<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<meta content="text/css" http-equiv="Content-Style-Type"/>
<meta content="pandoc" name="generator"/>
<title></title>
<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; }
code > span.dt { color: #902000; }
code > span.dv { color: #40a070; }
code > span.bn { color: #40a070; }
code > span.fl { color: #40a070; }
code > span.ch { color: #4070a0; }
code > span.st { color: #4070a0; }
code > span.co { color: #60a0b0; font-style: italic; }
code > span.ot { color: #007020; }
code > span.al { color: #ff0000; font-weight: bold; }
code > span.fu { color: #06287e; }
code > span.er { color: #ff0000; font-weight: bold; }
  </style>
<link href="../style/bootstrap.css" rel="stylesheet">
<link href="http://ajax.googleapis.com/ajax/libs/jqueryui/1.8.21/themes/black-tie/jquery-ui.css" rel="stylesheet">
<link href="../style/jquery.tocify.css" rel="stylesheet">
<link href="../style/prettify.css" rel="stylesheet" type="text/css"/>
<link href="../style/style.css" rel="stylesheet" type="text/css"/>
<link href="http://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic,700italic|Merriweather:400,300,300italic,400italic,700,900,700italic,900italic" rel="stylesheet" type="text/css">
<link href="../octicons/octicons.css" rel="stylesheet">
<style>  
  .headerDoc {
        color: #005580;
      }
  }
  </style>
</link></link></link></link></link></head>
<body>
<span id="side-nav">
<span class="side-nav-button chapterBtn mega-octicon octicon-book"></span>
<span class="side-nav-button navBtn mega-octicon octicon-three-bars"></span>
<span class="side-nav-button infoBtn mega-octicon octicon-info"></span>
</span>
<!-- table of content -->
<div id="toc">
<span class="closeBtn mega-octicon octicon-x"></span>
</div></body></html>
<!-- table of content -->
<div id="chapters">
<span class="closeBtn mega-octicon octicon-x"></span>
chapters
<ul><li class="group">foreword</li><li><a href="foreword.html">Foreword</a></li><li class="group">basics</li><li><a href="of_philosophy.html">philosophy</a></li><li><a href="cplusplus_basics.html">C++ Language Basics</a></li><li><a href="setup_and_project_structure.html">OF structure</a></li><li><a href="intro_to_graphics.html">Graphics</a></li><li><a href="OOPs!.html">Ooops! = Object Oriented Programming + Classes</a></li><li class="group">approaches</li><li><a href="animation.html">Animation</a></li><li><a href="data_vis.html">Information Visualization Chapter</a></li><li><a href="game_design.html">Experimental Game Development in openFrameworks</a></li><li><a href="image_processing_computer_vision.html">Image Processing and Computer Vision</a></li><li class="group">i/o</li><li><a href="hardware.html">hardware</a></li><li class="selected"><a href="sound.html">Sound</a></li><li><a href="network.html">Network</a></li><li class="group">advanced topics</li><li><a href="advanced_graphics.html">Advanced graphics</a></li><li><a href="math.html">That Math Chapter: From 1D to 4D</a></li><li><a href="memory.html">Memory in C++</a></li><li><a href="threads.html">Threads</a></li><li><a href="ios.html">ofxiOS</a></li><li><a href="c++11.html">C++ 11</a></li><li class="group">project breakdowns</li><li><a href="project_elliot.html">Case Study : Line Segments Space</a></li><li><a href="project_eva.html">Case Study: Choreographies for Humans and Stars</a></li><li><a href="project_joel.html">Case Study: Anthropocene, an interactive film installation for Greenpeace as part of their field at Glastonbury 2013</a></li><li class="group">tools</li><li><a href="version_control_with_git.html">Version control with Git</a></li><li><a href="ofSketch.html">ofSketch</a></li><li><a href="installation_up_4evr_macosx.html">Installation up 4evr - OSX</a></li><li><a href="installation_up_4evr_linux.html">Installation up 4evr - Linux</a></li></ul></div>
<!-- info about the book -->
<div id="bookInfo">
<span class="closeBtn mega-octicon octicon-x"></span>
Lorem ipsum dolor sit amet, consectetur adipiscing elit. Quisque at ante gravida, luctus mauris in, congue ante. Phasellus egestas est id velit ornare, eu vehicula augue tincidunt. Aliquam laoreet faucibus turpis eget gravida. Vestibulum pharetra scelerisque leo, eu tincidunt leo tincidunt feugiat. Sed rutrum, felis eget posuere tempus, neque justo pretium libero, quis facilisis mauris turpis nec enim. Sed quis metus massa. Maecenas tincidunt, leo at ultricies interdum, nisl ipsum finibus massa, quis dictum nulla nibh sed urna. Vestibulum rhoncus, nunc sit amet tincidunt interdum, ligula lacus dictum massa, pharetra tristique nisi augue vitae risus.
</div>
<div class="row-fluid">
<div id="chapter">
<h1 id="sound">Sound</h1>
<p><em>by <a href="http://adamcarlucci.com" target="_blank">Adam Carlucci</a></em></p>
<p>This chapter will demonstrate how to use the sound features that you'll find in openFrameworks, as well as some techniques you can use to generate and process sound.</p>
<p>Here's a quick overview of the classes you can use to work with sound in openFrameworks:</p>
<p><code>ofSoundPlayer</code> provides simple access to sound files, allowing you to easily load and play sounds, add sound effects to an app and extract some data about the file's sound as it's playing.</p>
<p><code>ofSoundStream</code> gives you access to the computer's sound hardware, allowing you to generate your own sound as well as react to sound coming into your computer from something like a microphone or line-in jack.</p>
<p>As of this writing, these classes are slated to be introduced in the next minor OF version (0.9.0):</p>
<p><code>ofSoundBuffer</code> is used to store a sequence of <code>float</code> values, and perform audio-related things on said values (like resampling)</p>
<p><code>ofSoundFile</code> allows you to extract uncompressed ofSoundBuffers from files.</p>
<p><code>ofSoundObject</code> is an interface for chaining bits of sound code together, similar to how a guitarist might use guitar pedals. This is mostly relevant for addon authors or people looking to share their audio processing code.</p>
<h2 id="getting-started-with-sound-files">Getting Started With Sound Files</h2>
<p>Playing a sound file is only a couple lines of code in openFrameworks. Just point an <code>ofSoundPlayer</code> at a file stored in your app's data folder and tell it to play.</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp"><span class="kw">class</span> ofApp : <span class="kw">public</span> ofBaseApp {
  ...
  ofSoundPlayer soundPlayer;
};

<span class="dt">void</span> ofApp::setup() {
  soundPlayer.loadSound(<span class="st">"song.mp3"</span>);
  soundPlayer.play();
}</code></pre>
<p>This is fine for adding some background music or ambiance to your app, but ofSoundPlayer comes with a few extra features that are particularly handy for handling sound effects.</p>
<p>"Multiplay" allows you to have a file playing several times simultaneously. This is great for any sound effect which might end up getting triggered rapidly, so you don't get stuck with an unnatural cutoff as the player's playhead abruptly jumps back to the beginning of the file. Multiplay isn't on by default. Use <code>soundPlayer.setMultiPlay(true)</code> to enable it. Then you can get natural sound effect behaviour with dead-simple trigger logic like this:</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp"><span class="kw">if</span> ( thingHappened )
  soundPlayer.play();
}</code></pre>
<p>Another feature built-in to ofSoundPlayer is speed control. If you set the speed faster than normal, the sound's pitch will rise accordingly, and vice-versa (just like a vinyl record). Playback speed is defined relative to "1", so "0.5" is half speed and "2" is double speed.</p>
<p>Speed control and multiplay are made for each other. Making use of both simultaneously can really extend the life of a single sound effect file. Every time you change a sound player's playback speed with multiplay enabled, previously triggered sound effects continue on unaffected. So, by extending the above trigger logic to something like...</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp"><span class="kw">if</span>( thingHappened ) {
  soundPlayer.setSpeed(ofRandom(<span class="fl">0.</span><span class="dv">8</span>, <span class="fl">1.</span><span class="dv">2</span>));
  soundPlayer.play();
}</code></pre>
<p>...you'll introduce a bit of unique character to each instance of the sound.</p>
<p>One other big feature of ofSoundPlayer is easy spectrum access. On the desktop platforms, you can make use of ofSoundGetSpectrum() to get the <em>frequency domain</em> representation of the sound coming from all of the currently active ofSoundPlayers in your app. An explanation of the frequency domain is coming a little later in this chapter, but running the openFrameworks <em>soundPlayerFFTExample</em> will give you the gist.</p>
<p>Ultimately, ofSoundPlayer is a tradeoff between ease-of-use and control. You get access to multiplay and pitch-shifted playback but you don't get extremely precise control or access to the individual samples in the sound file. For this level of control, ofSoundStream is the tool for the job.</p>
<h2 id="getting-started-with-the-sound-stream">Getting Started With the Sound Stream</h2>
<p>ofSoundStream is the gateway to the audio hardware on your computer, such as the microphone and the speakers. If you want to have your app react to live audio input or generate sound on the fly, this is the section for you!</p>
<p>You may never have to use the ofSoundStream directly, but it's the object that manages the resources needed to trigger <code>audioOut()</code> and <code>audioIn()</code> on your app. These two functions are optional members of your ofApp, like <code>keyPressed()</code>, <code>windowResized()</code> and <code>mouseMoved()</code>. They will start being called once you implement them and initiate the sound stream. Here's the basic structure for a sound-producing openFrameworks app:</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp"><span class="kw">class</span> ofApp : <span class="kw">public</span> ofBaseApp {
  ...
  <span class="dt">void</span> audioOut( <span class="dt">float</span> * output, <span class="dt">int</span> bufferSize, <span class="dt">int</span> nChannels );
  <span class="dt">double</span> phase;
}

<span class="dt">void</span> ofApp::setup() {
  phase = <span class="dv">0</span>;
  ofSoundStreamSetup(<span class="dv">2</span>, <span class="dv">0</span>); <span class="co">// 2 output channels (stereo), 0 input channels</span>
}

<span class="dt">void</span> ofApp::audioOut( <span class="dt">float</span> * output, <span class="dt">int</span> bufferSize, <span class="dt">int</span> nChannels ) {
  <span class="kw">for</span>(<span class="dt">int</span> i = <span class="dv">0</span>; i &lt; bufferSize * nChannels; i+=<span class="dv">2</span>) {
    <span class="dt">float</span> sample = sin(phase); <span class="co">// generating a sine wave sample</span>
    output[i] = sample; <span class="co">// writing to the left channel</span>
    output[i<span class="dv">+1</span>] = sample; <span class="co">// writing to the right channel</span>
    phase += <span class="fl">0.</span><span class="bn">05</span>;
  }
}</code></pre>
<p>When producing or receiving audio, the format is floating point numbers between -1 and 1 (the reason for this is coming a little later in this chapter). The sound will arrive in your app in the form of <em>buffers</em>. Buffers are just arrays, but the term "buffer" implies that each time you get a new one, it represents the chunk of time after the previous buffer. The reason openFrameworks asks you for buffers (instead of individual samples) is due to the overhead involved in shuttling data from your program to the audio hardware, and is a little outside the scope of this book.</p>
<p>The buffer size is adjustable, but it's usually a good idea to leave it at the default. The default isn't any number in particular, but will usually be whatever the hardware on your computer prefers. In practice, this is probably about 512 samples per buffer (256 and 1024 are other common buffer sizes).</p>
<p>Sound buffers in openFrameworks are <em>interleaved</em> meaning that the samples for each channel are right next to each other, like:</p>
<pre><code>[Left] [Right] [Left] [Right] ...</code></pre>
<p>This means you access individual sound channels in much the same way as accessing different colours in an ofPixels object (i.e. <code>buffer[i]</code> for the left channel, <code>buffer[i + 1]</code> for the right channel). The total size of the buffer you get in <code>audioIn()</code> / <code>audioOut()</code> can be calculated with <code>bufferSize * nChannels</code>.</p>
<p>An important caveat to keep in mind when dealing with ofSoundStream is that audio callbacks like <code>audioIn()</code> and <code>audioOut()</code> will be called on a seperate <em>thread</em> from the standard <code>setup()</code>, <code>update()</code>, <code>draw()</code> functions. This means that if you'd like to share any data between (for example) <code>update()</code> and <code>audioOut()</code>, you need to make use of an <code>ofMutex</code> to keep both threads from getting in each others' way. You can see this in action a little later in this chapter, or check out the threads chapter for a more in-depth explanation.</p>
<h2 id="why--1-to-1">Why -1 to 1?</h2>
<p>In order to understand <em>why</em> openFrameworks chooses to represent sound as a continuous stream of <code>float</code> values ranging from -1 to 1, it'll be helpful to know how sound is created on a physical level.</p>
<p><em>[ a minimal picture showing the mechanics of a speaker, reference: http://wiki.backyardbrains.com/images/5/54/Exp5_fig7.jpg ]</em></p>
<p>At the most basic level, a speaker consists of a cone and an electromagnet. The electromagnet pushes and pulls the cone to create vibrations in air pressure. These vibrations make their way to your ears, where they are interpreted as sound. When the electromagnet is off, the cone is simply "at rest", neither pulled in or pushed out.</p>
<p>[footnote] A basic microphone works much the same way: allowing air pressure to vibrate an object held in place by a magnet, thereby creating an electrical signal.</p>
<p>From the perspective of an openFrameworks app, it's not important what the sound hardware's specific voltages are. All that really matters is that the speaker cone is being driven between its "fully pushed out" and "fully pulled in" positions, which are represented as 1 and -1. This is similar to the notion of "1" as a representation of 100% as described in the animation chapter, though sound introduces the concept of -100%.</p>
<p>[footnote] Some other systems use an integer-based representation, moving between something like -65535 and +65535 with 0 still being the representation of "at rest". The Web Audio API provides an unsigned 8-bit representation, which ranges between 0 and 255 with 127 being "at rest".</p>
<p>A major way that sound differs from visual content is that there isn't really a "static" representation of sound. For example, if you were dealing with an OpenGL texture which represents 0 as "black" and 1 as "white", you could fill the texture with all 0s or all 1s and end up with a static image of "black" or "white" respectively. This is not the case with sound. If you were to create a sound buffer of all 0s, all 1s, all -1s, or any single number, they would all sound like exactly the same thing: nothing at all.</p>
<p>[footnote] Technically, you'd probably hear a pop right at the beginning as the speaker moves from the "at rest" position to whatever number your buffer is full of, but the remainder of your sound buffer would just be silence.</p>
<p>This is because what you actually hear is the <em>changes</em> in values over time. Any individual sample in a buffer doesn't really have a sound on its own. What you hear is the <em>difference</em> between the sample and the one before it. For instance, a sound's "loudness" isn't necessarily related to how "big" the individual numbers in a buffer are. A sine wave which oscillates between 0.9 and 1.0 is going to be much much quieter than one that oscillates between -0.5 and 0.5.</p>
<h2 id="time-domain-vs-frequency-domain">Time Domain vs Frequency Domain</h2>
<p>When representing sound as a continuous stream of values between -1 and 1, you're working with sound in what's known as the "Time Domain". This means that each value you're dealing with is referring to a specific moment in time. There is another way of representing sound which can be very helpful when you're using sound to drive some other aspect of your app. That representation is known as the "Frequency Domain".</p>
<p><em>[ image of a waveform vs an FFT bar graph, reference http://upload.wikimedia.org/wikipedia/commons/8/8c/Time_domain_to_frequency_domain.jpg ]</em></p>
<p>In the frequency domain, you'll be able to see how much of your input signal lies in various frequencies, split into separate "bins" (see above image).</p>
<p>You can transform a signal from the time domain to the frequency domain by a ubiquitous algorithm called the Fast Fourier Transform. You can get an openFrameworks-ready implementation of the FFT (along with examples!) in either the ofxFFT or ofxFft addons (by Lukasz Karluk and Kyle McDonald respectively).</p>
<p>In an FFT sample, bins in the higher indexes will represent higher pitched frequencies (i.e. treble) and the lower ones will represent bassy frequencies. Exactly <em>which</em> frequency is represented by each bin depends on the number of time-domain samples that went into the transform. You can calculate this as follows:</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp">frequency = (binIndex * sampleRate) / totalSampleCount</code></pre>
<p>So, if you were to run an FFT on a buffer of 1024 time domain samples at 44100Hz, bin 3 would represent 129.2Hz ( <code>(3 * 44100) / 1024 â‰ˆ 129.2</code> ). This calculation demonstrates a property of the FFT that is very useful to keep in mind: the more time domain samples you use to calculate the FFT, the better frequency resolution you'll get (as in, each subsequent FFT bin will represent frequencies that are closer together). The tradeoff for increasing the frequency resolution this way is that you'll start losing track of time, since your FFT will be representing a bigger portion of the signal.</p>
<p>Note: A raw FFT sample will typically represent its output as <a href="http://en.wikipedia.org/wiki/Complex_number" target="_blank">Complex numbers</a>, though this probably isn't what you're after if you're attempting to do something like audio visualization. A more intuitive representation is the <em>magnitude</em> of each complex number, which is calculated as:</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp">magnitude = sqrt( pow(complex.real, <span class="dv">2</span>) + pow(complex.imaginary, <span class="dv">2</span>) )</code></pre>
<p>If you're working with an FFT implementation that gives you a simple array of float values, it's most likely already done this calculation for you.</p>
<p>You can also transform a signal from the frequency domain <em>back</em> to the time domain, using an Inverse Fast Fourier Transform (aka IFFT). This is less common, but there is an entire genre of audio synthesis called Additive Synthesis which is built around this principle (generating values in the frequency domain then running an IFFT on them to create synthesized sound).</p>
<p>The frequency domain is useful for many things, but one of the most straightforward is isolating particular elements of a sound by frequency range, such as instruments in a song. Another common use is analyzing the character or timbre of a sound, in order to drive complex audio-reactive visuals.</p>
<p>The math behind the Fourier transform is a bit tricky, but it is fairly straightforward once you get the concept. I felt that <a href="http://betterexplained.com/articles/an-interactive-guide-to-the-fourier-transform/" target="_blank">this explanation of the Fourier Transform</a> does a great job of demonstrating the underlying math, along with some interactive visual examples.</p>
<h2 id="reacting-to-live-audio">Reacting to Live Audio</h2>
<h3 id="rms">RMS</h3>
<p>One of the simplest ways to add audio-reactivity to your app is to calculate the RMS of incoming buffers of audio data. RMS stands for "root mean square" and is a pretty straightforward calculation that serves as a good approximation of "loudness" (much better than something like averaging the buffer or picking the maximum value). The "square" step of the algorithim will ensure that the output will always be a positive value. This means you can ignore the fact that the original audio may have had "negative" samples (since they'd sound just as loud as their positive equivalent, anyway). You can see RMS being calculated in the <em>audioInputExample</em>.</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp"><span class="co">// modified from audioInputExample</span>
<span class="dt">float</span> rms = <span class="fl">0.</span><span class="dv">0</span>;
<span class="dt">int</span> numCounted = <span class="dv">0</span>;

<span class="kw">for</span> (<span class="dt">int</span> i = <span class="dv">0</span>; i &lt; bufferSize; i++){
    <span class="dt">float</span> leftSample = input[i * <span class="dv">2</span>] * <span class="fl">0.</span><span class="dv">5</span>;
    <span class="dt">float</span> rightSample = input[i * <span class="dv">2</span> + <span class="dv">1</span>] * <span class="fl">0.</span><span class="dv">5</span>;
    
    rms += leftSample * leftSample;
    rms += rightSample * rightSample;
    numCounted += <span class="dv">2</span>;
}

rms /= (<span class="dt">float</span>)numCounted;
rms = sqrt(rms);
<span class="co">// rms is now calculated</span></code></pre>
<h3 id="onset-detection-aka-beat-detection">Onset Detection (aka Beat Detection)</h3>
<p>Onset detection algorithms attempt to locate moments in an audio stream where an <em>onset</em> occurs, which is usually something like an instrument playing a note or the impulse of a drum hit. There are many onset detection algorithms available at various levels of complexity and accuracy, some fine-tuned for speech as opposed to music, some working in the frequency domain instead of the time domain, some made for offline processing as opposed to realtime, etc.</p>
<p>A simple realtime onset detection algorithm can be built on top of the RMS calculation above.</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp"><span class="kw">class</span> <span class="kw">class</span> ofApp : <span class="kw">public</span> ofBaseApp {
    ...
    <span class="dt">float</span> threshold;
    <span class="dt">float</span> minimumThreshold;
    <span class="dt">float</span> decayRate;
}

<span class="dt">void</span> ofApp::setup() {
    ...
    decayRate = <span class="fl">0.</span><span class="bn">05</span>;
    minimumThreshold = <span class="fl">0.</span><span class="dv">1</span>;
    threshold = minimumThreshold;
}

<span class="dt">void</span> ofApp::audioIn(<span class="dt">float</span> * input, <span class="dt">int</span> bufferSize, <span class="dt">int</span> nChannels) {
    ...
    threshold = ofLerp(threshold, minimumThreshold, decayRate);

    <span class="kw">if</span>(rms &gt; threshold) {
        <span class="co">// onset detected!</span>
        threshold = rms;
    }
}</code></pre>
<p>This will probably work fine on an isolated drum track, sparse music or for something like detecting whether or not someone's speaking into a microphone. However, in practice you'll likely find that this won't really cut it for reliable audio visualization or more intricate audio work.</p>
<p>You could of course grab an external onset detection algorithm (there's quite a few addons available for it), but if you'd like to experiment, try incorporating the FFT into your algorithm. For instance, try swapping the RMS for the average amplitude of a range of FFT bins.</p>
<h3 id="fft">FFT</h3>
<p>Running an FFT on your input audio will give you back a buffer of values representing the input's frequency content. A straight up FFT <em>won't</em> tell you which notes are present in a piece of music, but you will be able to use the data to take the input's sonic "texture" into account. For instance, the FFT data will let you know how much "bass" / "mid" / "treble" there is in the input at a pretty fine granularity (a typical FFT used for realtime audio-reactive work will give you something like 512 to 4096 individual frequency bins to play with).</p>
<p>When using the FFT to analyze music, you should keep in mind that the FFT's bins increment on a <em>linear</em> scale, whereas humans interpret frequency on a <em>logarithmic</em> scale. So, if you were to use an FFT to split a musical signal into 512 bins, the lowest bins (bin 0 through bin 40 or so) will probably contain the bulk of the data, and the remaining bins will mostly just be high frequency content. If you were to isolate the sound on a bin-to-bin basis, you'd be able to easily tell the difference between the sound of bins 3 and 4, but bins 500 and 501 would probably sound exactly the same. Unless you had robot ears.</p>
<p>[footnote] There's another transform called the <em>Constant Q Transform</em> (aka CQT) that is similar in concept to the FFT, but spaces its bins out logarithmically which is much more intuitive when dealing with music. As of this writing I'm not aware of any openFrameworks-ready addons for the CQT, but it's worth keeping in mind if you feel like pursuing other audio visualization options beyond the FFT.</p>
<h2 id="synthesizing-audio">Synthesizing Audio</h2>
<p>This section will walk you through the creation of a basic musical synthesizer. A full blown instrument is outside the scope of this book, but here you'll be introduced to the basic building blocks of synthesized sound.</p>
<p>A simple synthesizer can be implemented as a <em>waveform</em> modulated by an <em>envelope</em>, forming a single <em>oscillator</em>. A typical "real" synthesizer will have several oscillators and will also introduce <em>filters</em>, but many synthesizers at their root are variations on the theme of a waveform + envelope combo.</p>
<h3 id="waveforms">Waveforms</h3>
<p>Your synthesizer's waveform will define the oscillator's "timbre". The closer the waveform is to a sine wave, the more "pure" the resulting tone will be. A waveform can be made of just about anything, and many <a href="http://en.wikipedia.org/wiki/Category:Sound_synthesis_types" target="_blank">genres of synthesis</a> revolve around techniques for generating and manipulating waveforms.</p>
<p>A common technique for implementing a waveform is to create a <em>Lookup Table</em> containing the full waveform at a certain resolution. A <em>phase</em> index is used to scan through the table, and the speed that the phase index is incremented determines the pitch of the oscillator.</p>
<p>Here's a starting point for a synthesizer app that we'll keep expanding upon during this section. It demonstrates the lookup table technique for storing a waveform, and also visualizes the waveform and resulting audio output. You can use the mouse to change the resolution of the lookup table as well as the rendered frequency.</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp"><span class="kw">class</span> ofApp : <span class="kw">public</span> ofBaseApp {
<span class="kw">public</span>:
    <span class="dt">void</span> setup();
    <span class="dt">void</span> update();
    <span class="dt">void</span> draw();
    
    <span class="dt">void</span> updateWaveform(<span class="dt">int</span> waveformResolution);
    <span class="dt">void</span> audioOut(<span class="dt">float</span> * output, <span class="dt">int</span> bufferSize, <span class="dt">int</span> nChannels);
    
    std::vector&lt;<span class="dt">float</span>&gt; waveform; <span class="co">// this is the lookup table</span>
    <span class="dt">double</span> phase;
    <span class="dt">float</span> frequency;

    ofMutex waveformMutex;
    ofPolyline waveLine;
    ofPolyline outLine;
};

<span class="dt">void</span> ofApp::setup() {
    phase = <span class="dv">0</span>;
    updateWaveform(<span class="dv">32</span>);
    ofSoundStreamSetup(<span class="dv">1</span>, <span class="dv">0</span>); <span class="co">// mono output</span>
}

<span class="dt">void</span> ofApp::update() {
    ofScopedLock waveformLock(waveformMutex);
    updateWaveform(ofMap(ofGetMouseX(), <span class="dv">0</span>, ofGetWidth(), <span class="dv">3</span>, <span class="dv">64</span>, <span class="kw">true</span>));
    frequency = ofMap(ofGetMouseY(), <span class="dv">0</span>, ofGetHeight(), <span class="dv">60</span>, <span class="dv">700</span>, <span class="kw">true</span>);
}

<span class="dt">void</span> ofApp::draw() {
    ofBackground(ofColor::black);
    ofSetLineWidth(<span class="dv">5</span>);
    ofSetColor(ofColor::lightGreen);
    outLine.draw();
    ofSetColor(ofColor::cyan);
    waveLine.draw();
}

<span class="dt">void</span> ofApp::updateWaveform(<span class="dt">int</span> waveformResolution) {
    waveform.resize(waveformResolution);
    waveLine.clear();
    
    <span class="co">// "waveformStep" maps a full oscillation of sin() to the size </span>
    <span class="co">// of the waveform lookup table</span>
    <span class="dt">float</span> waveformStep = (M_PI * <span class="fl">2.</span>) / (<span class="dt">float</span>) waveform.size();
    
    <span class="kw">for</span>(<span class="dt">int</span> i = <span class="dv">0</span>; i &lt; waveform.size(); i++) {
        waveform[i] = sin(i * waveformStep);
        
        waveLine.addVertex(ofMap(i, <span class="dv">0</span>, waveform.size() - <span class="dv">1</span>, <span class="dv">0</span>, ofGetWidth()),
                           ofMap(waveform[i], <span class="dv">-1</span>, <span class="dv">1</span>, <span class="dv">0</span>, ofGetHeight()));
    }
}

<span class="dt">void</span> ofApp::audioOut(<span class="dt">float</span> * output, <span class="dt">int</span> bufferSize, <span class="dt">int</span> nChannels) {
    ofScopedLock waveformLock(waveformMutex);
    
    <span class="dt">float</span> sampleRate = <span class="dv">44100</span>;
    <span class="dt">float</span> phaseStep = frequency / sampleRate;
    
    outLine.clear();
    
    <span class="kw">for</span>(<span class="dt">int</span> i = <span class="dv">0</span>; i &lt; bufferSize * nChannels; i += nChannels) {
        phase += phaseStep;
        <span class="dt">int</span> waveformIndex = (<span class="dt">int</span>)(phase * waveform.size()) % waveform.size();
        output[i] = waveform[waveformIndex];
        
        outLine.addVertex(ofMap(i, <span class="dv">0</span>, bufferSize - <span class="dv">1</span>, <span class="dv">0</span>, ofGetWidth()),
                          ofMap(output[i], <span class="dv">-1</span>, <span class="dv">1</span>, <span class="dv">0</span>, ofGetHeight()));
    }
}</code></pre>
<p>Once you've got this running, try experimenting with different ways of filling up the waveform table (the line with <code>sin(...)</code> in it inside <code>updateWaveform(...)</code>). For instance, a fun one is to replace that line with:</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp">waveform[i] = ofSignedNoise(i * waveformStep, ofGetElapsedTimef());</code></pre>
<p>This will get you a waveform that naturally evolves over time. Be careful to keep your waveform samples in the range -1 to 1, though, lest you explode your speakers and / or brain.</p>
<h3 id="envelopes">Envelopes</h3>
<p>We've got a drone generator happening now, but adding some volume modulation into the mix will really bring the sound to life. This will let the waveform be played like an instrument, or otherwise let it sound like it's a living being that reacts to events.</p>
<p>We can create a simple (but effective) envelope with <code>ofLerp(...)</code> by adding the following to our app:</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp"><span class="kw">class</span> ofApp : <span class="kw">public</span> ofBaseApp {
    ...
    <span class="dt">float</span> volume;
};

<span class="dt">void</span> ofApp::setup() {
    ...
    volume = <span class="dv">0</span>;
}

<span class="dt">void</span> ofApp::update() {
    ...
    <span class="kw">if</span>(ofGetKeyPressed()) {
        volume = ofLerp(volume, <span class="dv">1</span>, <span class="fl">0.</span><span class="dv">8</span>); <span class="co">// jump quickly to 1</span>
    } <span class="kw">else</span> {
        volume = ofLerp(volume, <span class="dv">0</span>, <span class="fl">0.</span><span class="dv">1</span>); <span class="co">// fade slowly to 0</span>
    }    
}

<span class="dt">void</span> ofApp::audioOut(<span class="dt">float</span> * output, <span class="dt">int</span> bufferSize, <span class="dt">int</span> nChannels) {
    ...
    output[i] = waveform[waveformIndex] * volume;
    ...
}</code></pre>
<p>Now, whenever you press a key the oscillator will spring to life, fading out gradually after the key is released.</p>
<p>The standard way of controlling an envelope is with a relatively simple state machine called an ADSR, for "Attack, Decay, Sustain, Release".</p>
<ul>
<li><strong>Attack</strong> is how fast the volume reaches its peak after a note is triggered</li>
<li><strong>Decay</strong> is how long it takes the volume to fall from the peak</li>
<li><strong>Sustain</strong> is the resting volume of the envelope, which stays constant until the note is released</li>
<li><strong>Release</strong> is how long it takes the volume to drop back to 0 after the note is released</li>
</ul>
<p>A full ADSR implementation is left as an exercise for the reader, though <a href="http://www.earlevel.com/main/2013/06/03/envelope-generators-adsr-code/" target="_blank">this example from earlevel.com</a> is a nice reference.</p>
<h3 id="frequency-control">Frequency Control</h3>
<p>You can probably tell where we're going, here. Now that the app is responding to key presses, we can use those key presses to determine the oscillator's frequency. We'll introduce a bit more <code>ofLerp(...)</code> here too to get a nice <em>legato</em> effect.</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp"><span class="kw">class</span> ofApp : <span class="kw">public</span> ofBaseApp {
    ...
    <span class="dt">void</span> keyPressed(<span class="dt">int</span> key);
    <span class="dt">float</span> frequencyTarget;
}

<span class="dt">void</span> ofApp::setup() {
    ...
    frequency = <span class="dv">0</span>;
    frequencyTarget = frequency;
}

<span class="dt">void</span> ofApp::update() {
    ...
    <span class="co">// replace the "frequency = " line from earlier with this</span>
    frequency = ofLerp(frequency, frequencyTarget, <span class="fl">0.</span><span class="dv">4</span>);
}

<span class="dt">void</span> ofApp::keyPressed(<span class="dt">int</span> key) {
    <span class="kw">if</span>(key == <span class="st">'z'</span>) {
        frequencyTarget = <span class="fl">261.</span><span class="dv">63</span>; <span class="co">// C</span>
    } <span class="kw">else</span> <span class="kw">if</span>(key == <span class="st">'x'</span>) {
        frequencyTarget = <span class="fl">293.</span><span class="dv">67</span>; <span class="co">// D</span>
    } <span class="kw">else</span> <span class="kw">if</span>(key == <span class="st">'c'</span>) {
        frequencyTarget = <span class="fl">329.</span><span class="dv">63</span>; <span class="co">// E</span>
    } <span class="kw">else</span> <span class="kw">if</span>(key == <span class="st">'v'</span>) {
        frequencyTarget = <span class="fl">349.</span><span class="dv">23</span>; <span class="co">// F</span>
    } <span class="kw">else</span> <span class="kw">if</span>(key == <span class="st">'b'</span>) {
        frequencyTarget = <span class="fl">392.</span><span class="bn">00</span>; <span class="co">// G</span>
    } <span class="kw">else</span> <span class="kw">if</span>(key == <span class="st">'n'</span>) {
        frequencyTarget = <span class="fl">440.</span><span class="bn">00</span>; <span class="co">// A</span>
    } <span class="kw">else</span> <span class="kw">if</span>(key == <span class="st">'m'</span>) {
        frequencyTarget = <span class="fl">493.</span><span class="dv">88</span>; <span class="co">// B</span>
    }
}</code></pre>
<p>Now we've got a basic, useable instrument!</p>
<div class="figure">
<div style="clear:both"><a href="..\images\sound/images/synthesis-example.png" target="_blank"><img alt="Synthesis" src="..\images\sound/images/synthesis-example.png" title="screenshot of the synthesizer built in this section, clearly showing the aliasing effect of a low-resolution waveform"/></a></div><span class="caption">Synthesis</span>
</div>
<p>A few things to try, if you'd like to explore further:</p>
<ul>
<li>Instead of using <code>keyPressed(...)</code> to determine the oscillator's frequency, use ofxMidi to respond to external MIDI messages. If you want to get fancy, try implementing pitch bend or use MIDI CC messages to control the frequency lerp rate.</li>
<li>Try filling the waveform table with data from an image, or from a live camera (<code>ofMap(...)</code> will be handy to keep your data in the -1 to 1 range)</li>
<li>Implement a <em>polyphonic</em> synthesizer. This is one which uses multiple oscillators to let you play more than one note at a time.</li>
<li>Keep several copies of the <code>phase</code> index, and use <code>ofSignedNoise(...)</code> to slightly modify the frequency they represent. Add each of the waveforms together in <code>output</code>, but average the result by the number of phases you're tracking.</li>
</ul>
<p>For example:</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp"><span class="dt">void</span> ofApp::audioOut(<span class="dt">float</span> * output, <span class="dt">int</span> bufferSize, <span class="dt">int</span> nChannels) {
    ofScopedLock waveformLock(waveformMutex);

    <span class="dt">float</span> sampleRate = <span class="dv">44100</span>;
    <span class="dt">float</span> t = ofGetElapsedTimef();
    <span class="dt">float</span> detune = <span class="dv">5</span>;

    <span class="kw">for</span>(<span class="dt">int</span> phaseIndex = <span class="dv">0</span>; phaseIndex &lt; phases.size(); phaseIndex++) {
        <span class="dt">float</span> phaseFreq = frequency + ofSignedNoise(phaseIndex, t) * detune;
        <span class="dt">float</span> phaseStep = phaseFreq / sampleRate;

        <span class="kw">for</span>(<span class="dt">int</span> i = <span class="dv">0</span>; i &lt; bufferSize * nChannels; i += nChannels) {
            phases[phaseIndex] += phaseStep;
            <span class="dt">int</span> waveformIndex = (<span class="dt">int</span>)(phases[phaseIndex] * waveform.size()) % waveform.size();
            output[i] += waveform[waveformIndex] * volume;
        }
    }
    
    outLine.clear();
    <span class="kw">for</span>(<span class="dt">int</span> i = <span class="dv">0</span>; i &lt; bufferSize * nChannels; i+= nChannels) {
        output[i] /= phases.size();
        outLine.addVertex(ofMap(i, <span class="dv">0</span>, bufferSize - <span class="dv">1</span>, <span class="dv">0</span>, ofGetWidth()),
                          ofMap(output[i], <span class="dv">-1</span>, <span class="dv">1</span>, <span class="dv">0</span>, ofGetHeight()));
    }
}</code></pre>
<h2 id="audio-gotchas">Audio Gotchas</h2>
<h3 id="popping">"Popping"</h3>
<p>When starting or ending playback of synthesized audio, you should try to quickly fade in / out the buffer, instead of starting or stopping abruptly. If you start playing back a buffer that begins like <code>[1.0, 0.9, 0.8...]</code>, the first thing the speaker will do is jump from the "at rest" position of 0 immediately to 1.0. This is a <em>huge</em> jump, and will probably result in a "pop" that's quite a bit louder than you were expecting (based on your computer's current volume).</p>
<p>Usually, fading in / out over the course of about 30ms is enough to eliminate these sorts of pops.</p>
<p>If you're getting pops in the middle of your playback, you can diagnose it by trying to find reasons why the sound might be very briefly cutting out (i.e. jumping to 0, resulting in a pop if the waveform was previously at a non-zero value).</p>
<h3 id="clipping-distortion">"Clipping" / Distortion</h3>
<p>If your samples begin to exceed the range of -1 to 1, you'll likely start to hear what's known as "clipping", which generally sounds like a grating, unpleasant distortion. Some audio hardware will handle this gracefully by allowing you a bit of leeway outside of the -1 to 1 range, but others will "clip" your buffers.</p>
<p><em>[ clipped waveform image, reference http://www.st-andrews.ac.uk/~www_pa/Scots_Guide/audio/clipping/fig1.gif ]</em></p>
<p>Assuming this isn't your intent, you can generally blame clipping on a misbehaving addition or subtraction in your code. A multiplication of any two numbers between -1 and 1 will always result in another number between -1 and 1.</p>
<p>If you <em>want</em> distortion, it's much more common to use a <a href="http://music.columbia.edu/cmc/musicandcomputers/chapter4/04_06.php" target="_blank">waveshaping algorithm</a> instead of trying to find a way to make clipping sound good.</p>
<h3 id="latency">Latency</h3>
<p>No matter what, sound you produce in your app will arrive at the speakers sometime after the event that triggered the sound. The total time of this round trip, from the event to your app to the speakers is referred to as <em>latency</em>.</p>
<p>In practice, this usually isn't a big deal unless you're working on something like a musical instrument with very tight reaction time requirements (a drum instrument, for instance). If you're finding that your app's sound isn't responsive enough, you can try lowering the buffer size of your ofSoundStream. Be careful, though! The default buffer size is typically the default because it's determined to be the best tradeoff between latency and reliability. If you use a smaller buffer size, you might experience "popping" (as explained above) if your app can't keep up with the extra-strict audio deadlines.</p>
</div>
</div>
<script src="../javascript/jquery-1.8.3.min.js"></script>
<script src="../javascript/jquery-ui-1.9.1.custom.min.js"></script>
<script src="../javascript/bootstrap.js"></script>
<script src="../javascript/jquery.tocify.js"></script>
<script src="../javascript/prettify.js"></script>
<script src="../javascript/tocOpenClose.js"></script>
<script>
        $(function() {

            var toc = $("#toc").tocify({
              selectors: "h2,h3,h4,h5",
              showAndHide: false
            }).data("toc-tocify");

            prettyPrint();
            $(".optionName").popover({ trigger: "hover" });

        });
    </script>
