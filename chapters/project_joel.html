<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<meta content="text/css" http-equiv="Content-Style-Type"/>
<meta content="pandoc" name="generator"/>
<title></title>
<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; }
code > span.dt { color: #902000; }
code > span.dv { color: #40a070; }
code > span.bn { color: #40a070; }
code > span.fl { color: #40a070; }
code > span.ch { color: #4070a0; }
code > span.st { color: #4070a0; }
code > span.co { color: #60a0b0; font-style: italic; }
code > span.ot { color: #007020; }
code > span.al { color: #ff0000; font-weight: bold; }
code > span.fu { color: #06287e; }
code > span.er { color: #ff0000; font-weight: bold; }
  </style>
<link href="../style/bootstrap.css" rel="stylesheet">
<link href="http://ajax.googleapis.com/ajax/libs/jqueryui/1.8.21/themes/black-tie/jquery-ui.css" rel="stylesheet">
<link href="../style/jquery.tocify.css" rel="stylesheet">
<link href="../style/prettify.css" rel="stylesheet" type="text/css"/>
<link href="../style/style.css" rel="stylesheet" type="text/css"/>
<link href="http://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic,700italic|Merriweather:400,300,300italic,400italic,700,900,700italic,900italic" rel="stylesheet" type="text/css">
<link href="../octicons/octicons.css" rel="stylesheet">
<style>  
  .headerDoc {
        color: #005580;
      }
  }
  </style>
</link></link></link></link></link></head>
<body>
<span id="side-nav">
<span class="side-nav-button chapterBtn mega-octicon octicon-book"></span>
<span class="side-nav-button navBtn mega-octicon octicon-three-bars"></span>
<span class="side-nav-button infoBtn mega-octicon octicon-info"></span>
</span>
<!-- table of content -->
<div id="toc">
<span class="closeBtn mega-octicon octicon-x"></span>
</div></body></html>
<!-- table of content -->
<div id="chapters">
<span class="closeBtn mega-octicon octicon-x"></span>
chapters
<ul><li class="group">foreword</li><li><a href="foreword.html">Foreword</a></li><li class="group">basics</li><li><a href="of_philosophy.html">philosophy</a></li><li><a href="cplusplus_basics.html">C++ Language Basics</a></li><li><a href="setup_and_project_structure.html">OF structure</a></li><li><a href="intro_to_graphics.html">Graphics</a></li><li><a href="OOPs!.html">Ooops! = Object Oriented Programming + Classes</a></li><li class="group">approaches</li><li><a href="animation.html">Animation</a></li><li><a href="data_vis.html">Information Visualization Chapter</a></li><li><a href="game_design.html">Experimental Game Development in openFrameworks</a></li><li><a href="image_processing_computer_vision.html">Image Processing and Computer Vision</a></li><li class="group">i/o</li><li><a href="hardware.html">hardware</a></li><li><a href="sound.html">Sound</a></li><li><a href="network.html">Network</a></li><li class="group">advanced topics</li><li><a href="advanced_graphics.html">Advanced graphics</a></li><li><a href="math.html">That Math Chapter: From 1D to 4D</a></li><li><a href="memory.html">Memory in C++</a></li><li><a href="threads.html">Threads</a></li><li><a href="ios.html">ofxiOS</a></li><li><a href="c++11.html">C++ 11</a></li><li class="group">project breakdowns</li><li><a href="project_elliot.html">Case Study : Line Segments Space</a></li><li><a href="project_eva.html">Case Study: Choreographies for Humans and Stars</a></li><li class="selected"><a href="project_joel.html">Case Study: Anthropocene, an interactive film installation for Greenpeace as part of their field at Glastonbury 2013</a></li><li class="group">tools</li><li><a href="version_control_with_git.html">Version control with Git</a></li><li><a href="ofSketch.html">ofSketch</a></li><li><a href="installation_up_4evr_macosx.html">Installation up 4evr - OSX</a></li><li><a href="installation_up_4evr_linux.html">Installation up 4evr - Linux</a></li></ul></div>
<!-- info about the book -->
<div id="bookInfo">
<span class="closeBtn mega-octicon octicon-x"></span>
Lorem ipsum dolor sit amet, consectetur adipiscing elit. Quisque at ante gravida, luctus mauris in, congue ante. Phasellus egestas est id velit ornare, eu vehicula augue tincidunt. Aliquam laoreet faucibus turpis eget gravida. Vestibulum pharetra scelerisque leo, eu tincidunt leo tincidunt feugiat. Sed rutrum, felis eget posuere tempus, neque justo pretium libero, quis facilisis mauris turpis nec enim. Sed quis metus massa. Maecenas tincidunt, leo at ultricies interdum, nisl ipsum finibus massa, quis dictum nulla nibh sed urna. Vestibulum rhoncus, nunc sit amet tincidunt interdum, ligula lacus dictum massa, pharetra tristique nisi augue vitae risus.
</div>
<div class="row-fluid">
<div id="chapter">
<h1 id="case-study-anthropocene-an-interactive-film-installation-for-greenpeace-as-part-of-their-field-at-glastonbury-2013">Case Study: Anthropocene, an interactive film installation for Greenpeace as part of their field at Glastonbury 2013</h1>
<h2 id="project-overview">Project Overview</h2>
<p><strong>Anthropocene</strong></p>
<p><em>Adjective</em></p>
<p><em>Relating to or denoting the current geological age, viewed as the period during which human activity has been been the dominant influence on climate and the environment.</em></p>
<p>To see the finished project as part of a wider video all about the Greenpeace Field at Glastonbury 2013, please see the YouTube link below:</p>
<p>http://youtu.be/LwokmKqT0og?t=1m12s</p>
<p>Or an exceprt from inside the dome here:</p>
<p>https://vimeo.com/97113402</p>
<p>All source code can be found here:</p>
<p>https://github.com/HellicarAndLewis/Anthropocene</p>
<h2 id="the-project">The Project</h2>
<h3 id="initial-brief-from-client">Initial Brief from Client</h3>
<p>On 9th April 2013 we were approached by Paul Earnshaw of Greenpeace about an installation as part of Greenpeace's Field at Glastonbury 2013, a large music festival in the South West of England. Another interaction design studio had previously been in place to create a five day experience due to go live for the public duration of the festival on the 25th of June, but a scheduling conflict had emerged that had meant that they had to reluctantly withdraw.</p>
<p>Paul already had a budget and a unique space picked out for the installation, a large geodesic dome:</p>
<div class="figure">
<div style="clear:both"><a href="..\images\project_joel/images/2013_04_11_GreenpeaceStructure.jpg" target="_blank"><img alt="External Shell Structure for Installation from Client" src="..\images\project_joel/images/2013_04_11_GreenpeaceStructure.jpg" title="External Shell Structure for Installation from Client"/></a></div><span class="caption">External Shell Structure for Installation from Client</span>
</div>
<h3 id="our-response">Our response</h3>
<p>We initially sought out a projector hire firm, who responded with a quote and a plan for a projection setup that met our requirements for maximum visual impact on a budget:</p>
<div class="figure">
<div style="clear:both"><a href="..\images\project_joel/images/2013_04_16_DomeWithProjector.jpg" target="_blank"><img alt="Initial Projection Plan from Projector Firm" src="..\images\project_joel/images/2013_04_16_DomeWithProjector.jpg" title="Initial Projection Plan from Projector Firm"/></a></div><span class="caption">Initial Projection Plan from Projector Firm</span>
</div>
<p>After some studio thinking, by 16th April we responded with the following document:</p>
<div class="figure">
<div style="clear:both"><a href="..\images\project_joel/images/2013_04_16_Greenpeace_HandL_01.jpg" target="_blank"><img alt="First Page of Presentation" src="..\images\project_joel/images/2013_04_16_Greenpeace_HandL_01.jpg" title="First Page of Presentation"/></a></div><span class="caption">First Page of Presentation</span>
</div>
<blockquote>
<p>We would like this installation to be a relaxing and immersive space. An oasis where the viewer can relax on bean bags looking up at a screen.</p>
<p>We will use a mix of existing Greenpeace footage and a generative sound-scape to build a beautiful abstraction of the Arctic.</p>
<p>We would like to project onto the ceiling of the space, using either a rectangular, square or circular projection surface. We will experiment with different projection &gt;shapes and see what fits best aesthetically as well as meeting the budget.</p>
<p>We would like to explore the following ideas within the imagery, sound and feeling of the space.</p>
<p>1: The space as a timepiece - trying to have a cycle of sunset, night and dawn - each lasting around five minutes and having a single interaction between the floor and &gt;ceiling that is explored graphically and interactively.</p>
<p>2: Kaleidoscopes, shattering or delaying or time stretching footage. Breaking it up into blocks of time. Arranging in grids, or having different delays in different parts. &gt;The possibility of peoples movement being mirrored into the video playback in interesting ways, playing with time.</p>
<p>3: Making an oasis away from the rest of the festival that would last around 15 minutes, but raise some points about how the cycle of seasons of the Arctic are being &gt;affected.</p>
<p>4: Generative audio - a four channel speaker system that adds depth and texture the visuals.</p>
</blockquote>
<p><img alt="Third Page of Presentation, Proposed Diagram" src="images/2013_04_16_Greenpeace_HandL_03.jpg" title="Third Page of Presentation, Proposed Diagram"/> <img alt="Fourth Page of Presentation, Visual Experimentation" src="images/2013_04_16_Greenpeace_HandL_04.jpg" title="Fourth Page of Presentation, Visual Experimentation"/> <img alt="Sixth Page of Presentation, Arctic Experiments" src="images/2013_04_16_Greenpeace_HandL_06.jpg" title="Sixth Page of Presentation, Arctic Experiments"/> <img alt="Tenth Page of Presentation, Kaleidoscope Imagery from previous installation" src="images/2013_04_16_Greenpeace_HandL_10.jpg" title="Tenth Page of Presentation, Kaleidoscope Imagery from previous installation"/></p>
<p>On April 30th, we recieved an email from Paul:</p>
<p>"..we would love you to implement your proposal in our main feature of the dome at Glastonbury Festival this year..."</p>
<p>We had the project! Now it was time to get real about the budget, and see if we could get some interesting musical collaborators...</p>
<p>During May, we concentrated on discovering what film footage was available, and finalising the production design and kit list. My business partner Pete Hellicar spent many hours working on the edit of the film, aiming to complete it while other negotiations continued.</p>
<h3 id="audio-negotiations">Audio negotiations</h3>
<p>On May 10th, Pete reached out to our friends at Warp Records to see if any of their artists would be interested in donating their music to the project, and by the nick of project time we had permission from several artists to use their sounds.</p>
<h3 id="supplier-change-final-budget-negotiations-and-interaction-plan">Supplier change, Final Budget Negotiations and Interaction Plan</h3>
<p>By the end of May, we had changed hardware suppliers to ones already working with Greenpeace on their field, and had found replacement kit for our production. After experimenting with a circular projection screen, we'd arrived at a traditional projector set-up within the dome - a large rectangular projection screen about halfway up the dome wall with seating arranged in front of it. We'd also reached a final budget, and I was ready to start coding.</p>
<p>Pete and I had arrived at a final interactive concept after some discussions with Paul, who stated that a "show time" of about 15 minutes was desirable - enough time to get a detailed message across, but not so long as to bore a casual visitor. Pete took his film edit to 15 minutes and had it approved by the Greenpeace team for pacing and content. We decided to use the Microsoft Kinect to allow the openFrameworks application to distort or effect the film footage in real time - based on viewers movements in front of the projection screen. To make the dome a bit more comfortable Paul arranged the donation of several jumbo size bean bags - meaning that visitors could lie comfortably and wave their hands in the air to interact with the film - we angled the Kinect to hopefully pick up unintended user interaction first, surprising users and gently guiding them to stand in front of the bean bags and use their whole bodies to interact. We knew we had to strike a balance between a pre-scripted show and a completely spontaneous one - so we decided on developing several visual looks which could be placed onto a time line for easy repetition, editing and playback. The aim was to get to a system with the reliability of a static linear film and the responsivity of a live "VJ" system - albeit one that used the viewers silhouette rather than pre-rendered matts to affect the edited Greenpeace film.</p>
<p>At the beginning of June 2014 we received the following image from Paul:</p>
<div class="figure">
<div style="clear:both"><a href="..\images\project_joel/images/2013_06_04_SiteFromPaul.jpg" target="_blank"><img alt="Site Visit by Client" src="..\images\project_joel/images/2013_06_04_SiteFromPaul.jpg" title="Site Visit by Client"/></a></div><span class="caption">Site Visit by Client</span>
</div>
<p>The site awaited us.</p>
<h3 id="interactive-background-to-delay-maps-and-the-possibility-of-generating-a-delay-map-from-the-kinect-depth-image">Interactive Background to Delay Maps, and the possibility of generating a Delay Map from the Kinect Depth Image</h3>
<p><img alt="We are the time. We are the famous. Created at Fabrica." src="images/FabricaWeAreTheTimeWeAreTheFamous.jpg" title="We are the time. We are the famous. Created at Fabrica."/> We are the time. We are the famous. Created at Fabrica. <img alt="Hereafter by United Visual Artists." src="images/UVAHereafter.jpg" title="Hereafter by United Visual Artists."/> Hereafter by United Visual Artists. <img alt="Feedback by Hellicar&amp;Lewis." src="images/HellicarAndLewisFeedback.jpg" title="Feedback by Hellicar&amp;Lewis."/> Feedback by Hellicar&amp;Lewis.</p>
<p>Pete started the project by doing some sketches in Photoshop of how two dimensional angular shapes could "shatter" video footage - I realised that we could make similar effects in real time by using a delay map. This brought me back to previous projects around varying video delay across a whole image - starting with <a href="http://www.benettongroup.com/40years-press/fabrica_yeux_ouverts.html" target="_blank">"We are the time. We are the famous"</a> at <a href="http://fabrica.it/" target="_blank">Fabrica</a>, continuing with <a href="http://uva.co.uk/work/hereafter" target="_blank">Hereafter</a> at <a href="http://uva.co.uk/" target="_blank">UnitedVisualArtists</a> and finally <a href="http://www.hellicarandlewis.com/the-roundhouse/" target="_blank">Feedback</a> at <a href="http://hellicarandlewis.com" target="_blank">Hellicar&amp;Lewis</a>. Many people have been interested in this area for some time, <a href="http://www.flong.com/" target="_blank">Golan Levin</a> has compiled a <a href="http://www.flong.com/texts/lists/slit_scan/" target="_blank">list of related works</a>.</p>
<p>A delay map is simply grey-scale image that is used in combination with a digital video file to decide how much the video file should be delayed on a per-pixel basis. In this projects case a white pixel in a certain position in the delay map meant that there would be zero delay on the corresponding pixel of the video file currently being played back. Conversely, a black pixel in the delay map image would mean the maximum frame delay on the corresponding pixel of the video file. I.e. a completely white delay map image would combine with a video file to play back with zero delay, whilst a black image would give a uniform maximum delay - a linear horizontal grey-scale gradient would give a gradated delay from 0 on the white side to maximum on the black side - with all divisions smoothly displayed in between.</p>
<p>Delay maps are a great way of allowing an art director to quickly "paint" several grey-scale images in Photoshop or some similar image editing program and see the effects of that map on any video file - abstracting away the technical details of the underlying video delay code. This approach of using imagery to control underlying code is a particularly effective technique - making new tools for Art Directors to interface with code using visual techniques rather than syntax and text heavy traditional software engineering techniques.</p>
<p>The breakthrough after this initial thinking was to try to think of what other grey-scale maps I had seen - the live depth image of the Kinect! This would allow peoples 3D silhouettes to create per pixel delay maps that would change in real-time as they moved in front of the 3D sensors of the Microsoft device. The addition of James Georges <a href="https://github.com/obviousjim/ofxSlitScan" target="_blank">ofxSlitScan</a> made swapping in and out static grey scale maps very simple, all I had to do was combine the depth map with his existing code on a frame by frame basis.</p>
<h3 id="actual-timeline">Actual Timeline</h3>
<p>Here are the folder names of all the folders in my GreenpeaceArcticGlastonbury2013 folder.</p>
<ul>
<li>2013_04_11_PlansAndContentFromGreenpeace</li>
<li>2013_04_16_ProjectorQuotes</li>
<li>2013_04_30_PeteQuoteAndIdeas</li>
<li>2013_05_08_GlastoOverviewPlan</li>
<li>2013_05_14_PetePlanAndTechList</li>
<li>2013_05_20_GuestList</li>
<li>2013_05_28_CrewDetailsFromPete</li>
<li>2013_05_29_addons</li>
<li>2013_05_29_addonsAfterPragmatism</li>
<li>2013_05_29_ofxGUIFromDevelopGitHubBranch</li>
<li>2013_05_31_AddMaps</li>
<li>2013_06_02_BaficInvoice</li>
<li>2013_06_03_PeteEffectsFromSomantics</li>
<li>2013_06_04_HomeHigherResForPete</li>
<li>2013_06_06_CallToActionScript</li>
<li>2013_06_12_CrewForFieldReadup</li>
<li>2013_06_12_Font</li>
<li>2013_06_12_GreenpeaceLogos</li>
<li>2013_06_12_MoreCrewBriefing</li>
<li>2013_06_13_HuntResult</li>
<li>2013_06_13_MoreDurationBits</li>
<li>2013_06_13_obviousJimAudioReactiveRing</li>
<li>2013_06_16_ofxTimelineVideo</li>
<li>2013_06_19_Singleton</li>
<li>2013_06_19_VoiceOverOutro</li>
<li>2013_06_20_CateringMenu</li>
<li>2013_06_20_NewAddonsToTry</li>
<li>2013_06_24_CodeForArtFromJeffTimesten</li>
<li>2013_06_24_DeadFlock</li>
<li>2013_06_24_newFilmAndAudio</li>
<li>2013_06_24_ofxAddonsOFXContourUtil</li>
<li>2013_07_31_Final50Invoice</li>
<li>2013_08_18_ThankYouFromGreenpeace</li>
</ul>
<h2 id="development">Development</h2>
<h3 id="development-hardware-and-software-setup">Development Hardware and Software setup</h3>
<p>MacBook Pro * 15-inch, Mid 2009 * Processor: 3.06 GHz Intel Core 2 Duo * Memory: 4 GB 1067 MHz DDR3 * Graphics: NVIDIA GeForce 9600M GT 512 MB</p>
<ul>
<li>XCode for Development</li>
<li>Chrome for Web Browsing</li>
<li>Sublime Text for logging</li>
</ul>
<h3 id="explanation-and-discussion-of-development-in-detail">Explanation and Discussion of Development in Detail</h3>
<h4 id="ofxkinect-as-a-possible-input-to-ofxslitscan">ofxKinect, as a possible input to ofxSlitScan</h4>
<p>One of the benefits of using a platform like openFrameworks is that when people do release extras or "addons" they inevitably interface with the core - interesting results can be found by thinking about how addons can interface with each other using the core as a bridge.</p>
<p>In ofxKinect and ofxSlitScan's case, both addons used the same type of data:</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp"><span class="dt">unsigned</span> <span class="dt">char</span>* getDepthPixels();       <span class="co">///&lt;</span> grayscale values <span class="co">//from ofxKinect.h</span></code></pre>
<p>and</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp"><span class="dt">void</span> setDelayMap(<span class="dt">unsigned</span> <span class="dt">char</span>* map, ofImageType type); <span class="co">//from ofxSlitScan.h</span></code></pre>
<p>So connecting them was simple:</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp">slitScan.setDelayMap(depthPixels); <span class="co">//from testApp::update() in testApp.cpp</span></code></pre>
<p>This kind of separation demonstrates encapsulation or the information hiding qualities of software - the utility of not having to know the specifics of the implementation of the functionality described, merely the inputs required and outputs produced.</p>
<p>http://en.wikipedia.org/wiki/Encapsulation_(object-oriented_programming) http://en.wikipedia.org/wiki/Information_hiding</p>
<h4 id="ofxslitscan-using-pngs-and-moving-to-generating-realtime-delay-maps-making-a-aurora">ofxSlitScan, using PNG's and moving to generating realtime delay maps, making a Aurora</h4>
<p>Starting is often the hardest thing to do with programming. To combat this, I try to do the stupidest, most obvious thing first and then build from there. In this project, I started by prototyping various looks using static PNGs - feeding new data into the examples provided with ofxSlitScan. The provided an easy sketching ability - combined with a paint program to quickly produce many input variations.</p>
<p>The next place to experiment was making the input real-time and interactive - using the blobs from a sliced section of the live Kinect depth image from ofxKinect. Drawing these simple blobs as an image allowed them to be inputted into ofxSlitscan on a frame by frame basis - producing a time warping effect over the playback of the film that Pete Hellicar edited for the project. As so often happens, when the input to the interaction becomes real-time it was far more engaging, which is exactly what we wanted users to do - see SLITSCANKINECTDEPTHGREY mode below for more details on the precise implementation, and in the other cases that follow.</p>
<p>What else could be done with the depth information applied to the delay map of the slit scan? Experiments with effecting the blobs outline yielded the SPIKYBLOBSLITSCAN mode. Using the input from the Kinect as an input to a paint simulator was something that I had worked on with Marek Bereza in the Somantics project - it made sense to try it as an input to a slitscan, as can be seen in the PAINT mode. This Paint mode made something that very much resembled the appearance of a human aurora when mixed with the beautiful Borealis footage that Pete Hellicar had sourced with the help of Greenpeace. SPARKLE mode was another example of a successful port from Somantics to Anthropocene.</p>
<p>Another good strategy for finding new interesting things is to feed the output of a system back into its input - this is demonstrated well by the visual feedback effects produced by using video frames as the delay maps back into their own history - implemented in SELFSLITSCAN mode.</p>
<h4 id="ofxbox2d-making-ice-previous-projects-with-todd-vanderlin">ofxBox2d, making ice, previous projects with Todd Vanderlin</h4>
<p>I had previously worked with Todd Vanderlin on the Feedback project, where we had experimented with using Box2D (via Todd's ofxBox2D) as a way of "shattering" live video. Feedback used a screen orientated in portrait mode that encouraged the repeating of familiar existing behaviour - moving the experience from a tech demo to a playful joyous one. Having earlier experimented with ice like static PNG's I knew that using real-time triangles from ofxBox2D would work well - this time I had the advantage via the Kinect of a slice of 3D space as input, something that Todd had to work much harder to simulate using only 2D live camera input in Feedback. This aspect of constantly improving novel hardware inputs means that previous work can often be revisited and explored.</p>
<h4 id="ofxtimeline-understanding-how-cuing-works">ofxTimeline, understanding how cuing works</h4>
<p>To combine the film and the various real-time effects, it was essential to develop a cuing system to allow different effects to combine with different scenes in a reliably repeatable way. I began by experimenting with Duration, but after emailing the author of the addon (see development notes above), it become apparent that ofxTimeline would be a much better fit for the project - a subset of Durations code base.</p>
<p>After dealing with Quicktime performance issues (see below), the main challenge was cuing the effects. The structure of how ofxTimeline passes messages meant that the signal to switch scenes would only be sent when the play-head passed over the cue - clicking to a point after a cue meant that the signal to switch scenes would not be despatched. Deadlines of other functionality meant that this couldn't be fixed in time for show time - meaning that show operators would have to be careful when shuffling playback during the show proper.</p>
<h4 id="ofxgui-running-the-latest-branch-from-github-multiple-input-methods-and-gui-addons">ofxGui, running the Latest branch from Github, multiple input methods and GUI addons</h4>
<p>I knew that I wanted to augment ofxTimelines interface with controls for the setup of the Kinect and other custom requirements for the project. Watching the GitHub development branch revealed the release of an official core GUI addon - something I wanted to experiment with, which meant that I had to switch from an official static release of OF to the live development branch via Github. The project ended up with multiple interfaces - two graphical ones (ofxTimeline and ofxKinect control mainly) and a keyboard based one (consisting mainly of single boolean switches together with playback and editing shortcuts). With further development, a unified GUI would be desirable, but development pressures meant it wasn't a priority.</p>
<h4 id="ofxopticalflowfarneback-making-a-polar-bear">ofxOpticalFlowFarneback, making a polar bear</h4>
<p>During development and testing, I realised a furry look could serve well for making people feel like they were polar bears. I had seen "spikey" outline looks before - all achieved by drawing normals along the circumference of a blob. I'd also experimented with optical flow in previous projects and started thinking about how the two could be combined - I looked for optical flow addons on <a href="http://ofxaddons.com" target="_blank">ofxaddons.com</a> and discovered a flurry of recent activity since I'd last checked. Development tends to flow like this - periods of fallow followed by simultaneous parallel development from several quarters.</p>
<ul>
<li><a href="https://github.com/Flightphase/ofxCvOpticalFlowLK" target="_blank">ofxCvOpticalFlowLK by James George</a></li>
<li><a href="https://github.com/timscaffidi/ofxOpticalFlowFarneback" target="_blank">ofxOpticalFlowFarneback by Tim Scaffidi</a></li>
<li><a href="https://github.com/julapy/ofxOpticalFlowLK" target="_blank">ofxOpticalFlowLK by Lukasz Karluk</a></li>
</ul>
<p>Tim Scaffidi's version immediately stood out to Pete, so I developed two simple colourings for Aurora and Polar Bear modes, merely tweaking Tim's excellent demo code.</p>
<h3 id="xml-issues-around-the-naming-of-scenes">XML Issues around the Naming of Scenes</h3>
<p>Mid development, I found that saving the XML wasn't functioning as expected - it turned out to be the fault of non alpha numeric characters in scene names. I learnt the hard way that it's always good to avoid punctuation and spaces altogether and use <a href="http://en.wikipedia.org/wiki/CamelCase" target="_blank">CamelCase</a>.</p>
<h3 id="video-performance-using-the-highperformanceexample">Video Performance, using the HighPerformanceExample</h3>
<p>Right from the beginning of the project, it was obvious that video decoding would be significant portion of processing time per frame. Others in the openFrameworks community had been investigating performance in recent years, with James George contributing an <a href="https://github.com/openframeworks/openFrameworks/commit/4e02db8d82c520bef6c09d58b37076a84fe37571" target="_blank">OSX only High Performance video example</a>. This used native Quicktime playback features, enabling far higher performance on compatible hardware. While this undoubted enabled the film playback to function smoothly, it did make the code less platform independent - one of the inevitable compromises that happens during development.</p>
<h3 id="counting-the-items-in-an-enum">Counting the items in an Enum</h3>
<p>I knew that I would have to switch between different visual looks as the film was played back by the program. C++ provides the ENUM keyword to allow the coder to define a data set of named elements, but I needed a way to count the number of modes programmatically. <a href="http://stackoverflow.com/questions/2102582/how-can-i-count-the-items-in-an-enum" target="_blank">Stack Overflow</a> provided the solution.</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp"><span class="kw">enum</span> GreenpeaceModes {BLANK, GUI, VIDEO, VIDEOCIRCLES, KINECTPOINTCLOUD, SLITSCANBASIC, SLITSCANKINECTDEPTHGREY, SPARKLE, VERTICALMIRROR, HORIZONTALMIRROR, KALEIDOSCOPE, COLOURFUR, DEPTH, SHATTER, SELFSLITSCAN, SPIKYBLOBSLITSCAN, MIRRORKALEIDOSCOPE, PARTICLES, WHITEFUR, PAINT, GreenpeaceModes_MAX = PAINT}; <span class="co">//best to use ALL CAPS for enumerated types and constants so you can tell them from ClassNames and variableNames. Use camelCase for variableNames - http://en.wikipedia.org/wiki/CamelCase</span>
 
<span class="co">/* http://stackoverflow.com/questions/2102582/how-can-i-count-the-items-in-an-enum</span>
<span class="co"> For C++, there are various type-safe enum techniques available, and some of those (such as the proposed-but-never-submitted Boost.Enum) include support for getting the size of a enum.</span>
<span class="co"> </span>
<span class="co"> The simplest approach, which works in C as well as C++, is to adopt a convention of declaring a ...MAX value for each of your enum types:</span>
<span class="co"> </span>
<span class="co"> enum Folders { FA, FB, FC, Folders_MAX = FC };</span>
<span class="co"> ContainerClass *m_containers[Folders_MAX + 1];</span>
<span class="co"> ....</span>
<span class="co"> m_containers[FA] = ...; // etc.</span>
<span class="co"> Edit: Regarding { FA, FB, FC, Folders_MAX = FC} versus {FA, FB, FC, Folders_MAX]: I prefer setting the ...MAX value to the last legal value of the enum for a few reasons:</span>
<span class="co"> </span>
<span class="co"> The constant's name is technically more accurate (since Folders_MAX gives the maximum possible enum value).</span>
<span class="co"> Personally, I feel like Folders_MAX = FC stands out from other entries out a bit more (making it a bit harder to accidentally add enum values without updating the max value, a problem Martin York referenced).</span>
<span class="co"> GCC includes helpful warnings like "enumeration value not included in switch" for code such as the following. Letting Folders_MAX == FC + 1 breaks those warnings, since you end up with a bunch of ...MAX enumeration values that should never be included in switch.</span>
<span class="co"> switch (folder)</span>
<span class="co"> {</span>
<span class="co"> case FA: ...;</span>
<span class="co"> case FB: ...;</span>
<span class="co"> // Oops, forgot FC!</span>
<span class="co"> }</span>
<span class="co">*/</span></code></pre>
<p>I used the Stack Overflow tip in the <code>void testApp::keyPressed (int key)</code> method.</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp"><span class="kw">case</span> <span class="st">'a'</span>: <span class="co">//used to be key left, but it interferes with ofxtimeline</span>
{
    currentMode = (GreenpeaceModes)((<span class="dt">int</span>)currentMode - <span class="dv">1</span>);
    <span class="kw">if</span>(currentMode &lt; <span class="dv">0</span>){
        currentMode = GreenpeaceModes_MAX;<span class="co">//see .h file for stackoverflow justification</span>
    }
    <span class="kw">break</span>;
}
<span class="kw">case</span> <span class="st">'s'</span>: <span class="co">//used to be key right, but it interferes with ofxtimeline</span>
{
    currentMode = (GreenpeaceModes)((<span class="dt">int</span>)currentMode + <span class="dv">1</span>);
    <span class="kw">if</span>(currentMode &gt; GreenpeaceModes_MAX){
        currentMode = (GreenpeaceModes)<span class="dv">0</span>;<span class="co">//see .h file for stackoverflow justification</span>
    }
}</code></pre>
<p>While I could have gone down the <a href="http://en.wikipedia.org/wiki/Polymorphism_(computer_science)" target="_blank">polymorphic</a> custom class route, I felt that the ENUM apporach provided good performance (through compiler optimisation of common C++ coding paradigms), speed of development (lower file overhead) and clarity of code.</p>
<h3 id="sequencing">Sequencing</h3>
<div class="figure">
<div style="clear:both"><a href="..\images\project_joel/images/2013_06_25_KieranProjection.jpg" target="_blank"><img alt="Kieran in front of the projection screen, final sequencing" src="..\images\project_joel/images/2013_06_25_KieranProjection.jpg" title="Kieran in front of the projection screen, final sequencing"/></a></div><span class="caption">Kieran in front of the projection screen, final sequencing</span>
</div>
<p>Kieran and Pete completed the main sequencing on-site.</p>
<h2 id="show-time">Show time</h2>
<div class="figure">
<div style="clear:both"><a href="..\images\project_joel/images/2013_06_27_HandsInTheAir.jpg" target="_blank"><img alt="The Installation in Action, 27th June 2013" src="..\images\project_joel/images/2013_06_27_HandsInTheAir.jpg" title="The Installation in Action, 27th June 2013"/></a></div><span class="caption">The Installation in Action, 27th June 2013</span>
</div>
<h2 id="post-event">Post Event</h2>
<p>The biggest PR boost to the project while it was live was a <a href="http://www.glastonburyfestivals.co.uk/news/greenpeace-at-glastonbury-2013" target="_blank">review</a> from Timeout:</p>
<p>"A highlight of the Greenpeace field was undoubtedly the Arctic Dome, voted by Time Out as the second best non-musical thing to do at the Festival and previewed by NME. It offered people the opportunity to disappear through a crack in the ice and take a magical 15-minute trip to the North Pole, where ice towered and the Northern Lights danced."</p>
<h3 id="testimony-from-show-operators">Testimony from Show Operators</h3>
<p>Kieran and Bafic were the people who ran the show for the general public, below is their testimony, with Kieran starting:</p>
<p><em>Did you have a routine before show time?</em></p>
<p>Before the first show of the day we’d double check the connection between the laptop and the Kinect and test with the skeleton tracking that everything was working correctly. Before show time we’d dim the lights, make sure the sound was turned on, switch to the point cloud setting so people could see themselves as they walked in and then we’d turn the beanbags upright so as to ’set the scene’. Then, as people started to walk in we’d raise the lights as though they were walking on stage. And then before we pressed play we’d dim the lights to black.</p>
<p><em>Any problems during shows? Crashes? Funny stories?</em></p>
<p>A couple of times the connection between the Kinect and the laptop failed due to the cable being under tension so we just had to let the show run to the end before we could fix it. The main problem we had was the projector overheating and displaying a lamp warning which involved having to find the technician to sort it. At one point the projector overheated so badly that we had to leave it switched off for 40 minutes before we could run the show again.</p>
<p>Off the top of my head I can’t think of anything I’d like to change about it, the GUI had quite a steep learning curve so it took a while to remember all the keys to press to hide each part of the interface but once we’d mastered that everything was fine. I guess the only thing that would be good but most likely ultimately un-achieveable would be full automation in the sense that the station wouldn’t have to be manned at all times.</p>
<p>Following is Bafic's post show report:</p>
<p><em>Did you have a routine before show time?</em></p>
<p>Before every show we would go through different ways to layout the bean bags. It's started off as just a small idea but as we kept on doing it we noticed that it would affect how people acted with the film. Some were semi circles some were bean bags set up in rows like cinema seats sometimes we pushed all bean bags to the back and told people they had to stand up and use their full body to interact with the film.</p>
<p>When seated in rows people mostly used their arms (a few people were moving the legs in air sitting down) but never was it a full body movement until we moved bean bags to the back . Some excited people would stand up and run to in front of the Kinect and interact with it that way, after they had finished they would sit down and someone else would follow due to the sheer curiosity of seeing what the previous person had done. It was interesting because everyone was so curious as to what would happen. I was sitting their amazed because their were a few loops/back and forths happening.</p>
<ol style="list-style-type: decimal">
<li>You had the back and forth between the one person who would stand up interact with the Kinect and then that would show up on the projection.</li>
<li>They would sit down and the next back and forth would be the next person to stand up start off with maybe replicating the previous persons techniques and movement AND Then coming up with the own ideas and movement.</li>
<li>then their was us who was watching and getting excited and seeing what they were doing and changing effects depending on what the user was doing and what we felt like could be interesting then obviously what we put on screen would effect how the person would dance/move/use their body. The whole thing was like a 3x over Möbius strip of events and occurrences that kept affecting the previous element and also the next element at the same time!</li>
</ol>
<p><em>Any problems during shows? Crashes? Funny stories?</em></p>
<p>I can't think of any crashes or problems that happened. Their was a time when someone came in with a puppet on a long stick and they waved it at the Kinect and that would egg on the rest of the audience because this funny puppet would appear on screen. The whole experience was really amazing and interesting.</p>
<h3 id="open-source-discussions-with-client">Open Source discussions with Client</h3>
<p>Greenpeace were happy for us to Open Source, as we do with all our projects. Greenpeace does not have a GitHub of it's own, but we were able to suggest that that should be part of their future strategy. The problem was the film that formed the backdrop for the interaction - while musicians were happy to license music for a live only experience, getting those rights in perpetuity has been challenging. Negotiations continue.</p>
<h3 id="re-running-remotely-in-australia-and-new-zealand">Re-running remotely in Australia and New Zealand</h3>
<p>The project has been re-exhibited twice in the Southern Hemisphere - in Australia and New Zealand. Getting the code up and running wasn't a problem - but training someone to use the two layers of mouse GUI and on layer of Keyboard GUI was a challenge, especially over a painfully slow Skype connection.</p>
<h3 id="future-development">Future development</h3>
<p>Paul Valery said 'Poems are never finished - just abandoned'. This is sadly true for all artistic endeavours. Below are three areas for future development.</p>
<h4 id="social-interaction">Social interaction</h4>
<p>The <a href="http://www.hellicarandlewis.com/the-hello-wall/" target="_blank">Hello Wall</a> and <a href="http://www.hellicarandlewis.com/tate-modern/" target="_blank">Hello Cube</a> projects showed how making feedback loops between users and installations via social networks is not only fun, but helps spread awareness of the installation beyond the physical bounds of the project. Imagine allowing users to post comments to the projection as it happening via Twitter and receiving bespoke screen grabs showing evidence of their interaction in return - or even choosing which of the interactive effects is active at a certain time. The meta data of these interactions could be used to come up with the most enaging timeline, or to deliver messages to users in the days, weeks and months following the installation - particularly useful for an organisation such as Greenpeace that relies on public support to lobby Governments and Corporations.</p>
<h4 id="broadcast">Broadcast</h4>
<p>Pete and I discussed how we could transform the installation into one that broadcast itself to a wider audience when we were in the planning stage. Unfortunately, securing a reliable Internet connection at the Glastonbury Music festival proved impossible. Post and Previous Hellicar&amp;Lewis projects for <a href="http://www.hellicarandlewis.com/nikefeeltv/" target="_blank">Nike</a> and <a href="http://www.hellicarandlewis.com/coke/" target="_blank">Coca-Cola</a> show how broadcasting an installation with the addition of social network interaction can dramatically increase engagement. We hope to be able to make such a socially activated broadcast interaction with Greenpeace in the near future - imagine several locations around the world witnessing the same film simultaneously with body movement from each location feeding back into the others - live video portals of depth maps crossing continents and time zones to produce a truly global event.</p>
<h4 id="raspberry-pi">Raspberry Pi</h4>
<p>With the advent of a <a href="http://www.openframeworks.cc/setup/raspberrypi/" target="_blank">Raspberry Pi</a> port of openFrameworks, a port of the project to the platform would allow for the deployment of the project to events that have even smaller budgets than this iteration. This would also entail a port of the Kinect code to 2D computer vision, but I'm confident this would be a spur for other interactions and visual effects.</p>
<h3 id="conclusion">Conclusion</h3>
<p>All in all, for a low budget project, using openFrameworks was the differentiator that enabled me to collaborate with the rest of the team at Hellicar&amp;Lewis to make the installation come to life. The key factors were being able to draw upon so many external addons, previous projects and the community as a whole.</p>
<h2 id="team-and-credits">Team and Credits</h2>
<ul>
<li>Pete Hellicar and Joel Gethin Lewis</li>
<li>Commissioned by Paul Earnshaw of Greenpeace</li>
<li>Produced by Sarah Toplis</li>
<li>Assisted by <a href="http://www.bafic.co.uk/" target="_blank">Bafic</a> and <a href="http://www.kieranstartup.co.uk/" target="_blank">Kieran Startup</a></li>
</ul>
<p>Project uses addons and other code Contributions from:</p>
<ul>
<li><a href="http://www.mazbox.com/" target="_blank">Marek Bereza aka Mazbox</a> as part of Cariad Interactive</li>
<li><a href="https://github.com/ofTheo/ofxKinect" target="_blank">ofxKinect</a> by <a href="http://www.theowatson.com/" target="_blank">Theo Watson</a></li>
<li><a href="https://github.com/obviousjim/ofxSlitScan" target="_blank">ofxSlitScan</a> by <a href="http://jamesgeorge.org/" target="_blank">James George</a></li>
<li><a href="https://github.com/vanderlin/ofxBox2d" target="_blank">ofxBox2d</a> by <a href="http://vanderlin.cc/" target="_blank">Todd Vanderlin</a></li>
<li><a href="https://github.com/YCAMInterlab/ofxTimeline" target="_blank">ofxTimeline</a> by <a href="http://jamesgeorge.org/" target="_blank">James George</a></li>
<li><a href="https://github.com/timscaffidi/ofxOpticalFlowFarneback" target="_blank">ofxOpticalFlowFarneback</a> by <a href="http://timothyscaffidi.com/" target="_blank">Tim Scaffidi</a></li>
</ul>
<p>Thanks to: * All our families and friends. * The Greenpeace Family * Microsoft for being Open * <a href="http://www.theowatson.com/" target="_blank">Theo Watson</a> * The entire openFrameworks community * <a href="http://marshmallowlaserfeast.com/" target="_blank">Marshmallow Laser Feast</a> * <a href="http://timothyscaffidi.com/" target="_blank">Tim Scaffidi</a> * <a href="http://jamesgeorge.org/" target="_blank">James George</a> * <a href="http://interlab.ycam.jp/en" target="_blank">YCAM InterLab</a></p>
<h2 id="hardware-selection">Hardware selection</h2>
<ul>
<li>1 x 3D Camera - Microsoft XBox360 Kinect</li>
<li>1 x Playback and Interaction Computer - MacBook Pro Retina</li>
<li>1 x 10K projector</li>
<li>1 x Projection Screen</li>
<li>Sound - 4 x D&amp;B T-10 Top + Amp 2 x Subs</li>
</ul>
<h2 id="appendix-1-code-structure-main-loop">Appendix 1: Code structure, main loop</h2>
<p>The structure of setup(), update() and draw() methods is common to openFrameworks code - with the addition of two large switch statements for switching between modes at runtime.</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp"><span class="co">//--------------------------------------------------------------</span>
<span class="dt">void</span> testApp::update() {
    <span class="co">//kinect</span>
    kinect.update();
    <span class="co">// there is a new frame and we are connected</span>
    <span class="kw">if</span>(kinect.isFrameNew()) {
        <span class="co">// load grayscale depth image from the kinect source</span>
        depthPreCrop.setFromPixels(kinect.getDepthPixels(), kinect.width, kinect.height);
        
        <span class="kw">if</span>(mirror){
            depthPreCrop.mirror(<span class="kw">false</span>, <span class="kw">true</span>);
        }
        
        maskGrayImage();
        depthPreCrop.flagImageChanged();

        <span class="co">// save original depth, and do some preprocessing</span>
        depthOrig = depthPreCrop; <span class="co">//copy cropped image into orig</span>
        depthProcessed = depthOrig; <span class="co">//copy orig into processed</span>
        colorImageRGB = kinect.getPixels(); <span class="co">//getting colour pixels</span>
        
        <span class="kw">if</span>(invert) depthProcessed.invert();
        <span class="kw">if</span>(mirror) {
            colorImageRGB.mirror(<span class="kw">false</span>, <span class="kw">true</span>);
            <span class="co">//greyIRSingleChannel.mirror(false, true);</span>
        }
        
        depthOrig.flagImageChanged();
        depthProcessed.flagImageChanged();
        colorImageRGB.flagImageChanged();

        <span class="kw">if</span>(preBlur) cvSmooth(depthProcessed.getCvImage(), depthProcessed.getCvImage(), CV_BLUR , preBlur*<span class="dv">2+1</span>);
        <span class="kw">if</span>(topThreshold) cvThreshold(depthProcessed.getCvImage(), depthProcessed.getCvImage(), topThreshold * <span class="dv">255</span>, <span class="dv">255</span>, CV_THRESH_TRUNC);
        <span class="kw">if</span>(bottomThreshold) cvThreshold(depthProcessed.getCvImage(), depthProcessed.getCvImage(), bottomThreshold * <span class="dv">255</span>, <span class="dv">255</span>, CV_THRESH_TOZERO);
        <span class="kw">if</span>(dilateBeforeErode) {
            <span class="kw">if</span>(dilateAmount) cvDilate(depthProcessed.getCvImage(), depthProcessed.getCvImage(), <span class="dv">0</span>, dilateAmount);
            <span class="kw">if</span>(erodeAmount) cvErode(depthProcessed.getCvImage(), depthProcessed.getCvImage(), <span class="dv">0</span>, erodeAmount);
        } <span class="kw">else</span> {
            <span class="kw">if</span>(erodeAmount) cvErode(depthProcessed.getCvImage(), depthProcessed.getCvImage(), <span class="dv">0</span>, erodeAmount);
            <span class="kw">if</span>(dilateAmount) cvDilate(depthProcessed.getCvImage(), depthProcessed.getCvImage(), <span class="dv">0</span>, dilateAmount);
        }
        depthProcessed.flagImageChanged();

        <span class="co">// find contours</span>
        depthContours.findContours(depthProcessed,
                                   minBlobSize * minBlobSize * depthProcessed.getWidth() * depthProcessed.getHeight(),
                                   maxBlobSize * maxBlobSize * depthProcessed.getWidth() * depthProcessed.getHeight(),
                                   maxNumBlobs, findHoles, useApproximation);
        <span class="co">//now do the diff bits for the PAINT mode</span>
        ofxCvGrayscaleImage thresholdedDepthImageForPaint;
        thresholdedDepthImageForPaint.setFromPixels(depthProcessed.getPixelsRef());
        thresholdedDepthImageForPaint.resize(paintCanvas.getWidth(), paintCanvas.getHeight());
        thresholdedDepthImageForPaint.flagImageChanged();
        <span class="co">// loop through pixels</span>
        <span class="co">//  - add new colour pixels into canvas</span>
        <span class="dt">unsigned</span> <span class="dt">char</span> *canvasPixels = paintCanvas.getPixels();
        <span class="dt">unsigned</span> <span class="dt">char</span> *diffPixels = thresholdedDepthImageForPaint.getPixels();

        <span class="dt">int</span> r = <span class="dv">255</span>;

        <span class="kw">for</span>(<span class="dt">int</span> i = <span class="dv">0</span>; i &lt; paintCanvas.width*paintCanvas.height; i++) {
            <span class="kw">if</span>(diffPixels[i]) {
                <span class="co">//paint in the new colour if</span>
                canvasPixels[i*<span class="dv">3</span>] = r;
                canvasPixels[i*<span class="dv">3+1</span>] = r;
                canvasPixels[i*<span class="dv">3+2</span>] = r;
            }<span class="kw">else</span>{
                <span class="dt">int</span> greyScale = (<span class="dt">int</span>)(canvasPixels[i*<span class="dv">3</span>]*<span class="fl">0.</span><span class="er">9f</span>);
                canvasPixels[i*<span class="dv">3</span>] = greyScale;
                canvasPixels[i*<span class="dv">3+1</span>] = greyScale;
                canvasPixels[i*<span class="dv">3+2</span>] = greyScale;
            }
        }
        paintCanvas.blur();
        paintCanvas.flagImageChanged();
        paintCanvasAsOfImage.setFromPixels(paintCanvas.getPixelsRef());
        paintCanvasAsOfImage.update();
        flowSolver.setPyramidScale(pyramidScale);
        flowSolver.setPyramidLevels(pyramidLevels);
        flowSolver.setWindowSize(windowSize);
        flowSolver.setExpansionArea(expansionAreaDoubleMe*<span class="dv">2</span>);
        flowSolver.setExpansionSigma(expansionSigma);
        flowSolver.setFlowFeedback(flowFeedback);
        flowSolver.setGaussianFiltering(gaussianFiltering);
        flowSolver.update(depthProcessed);
    }


    <span class="co">//Dirty filthy hack</span>
    <span class="kw">if</span>(currentMode != SLITSCANBASIC){
        prevSlitScan = <span class="dv">-1</span>;
    }
    <span class="kw">switch</span>(currentMode){</code></pre>
<p>see below for mode by mode update details</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp">        <span class="kw">default</span>:
            <span class="kw">break</span>;
    }
}</code></pre>
<pre class="sourceCode cpp"><code class="sourceCode cpp"><span class="dt">void</span> testApp::draw() {
	ofBackground(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>);
	ofSetColor(<span class="dv">255</span>, <span class="dv">255</span>, <span class="dv">255</span>);
    
    <span class="kw">switch</span> (currentMode) {</code></pre>
<p>see below for descriptions of various modes drawing</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp">    }
    
    <span class="kw">if</span>( bShowNonTimelineGUI ){
        nonTimelineGUI.draw();
    }

    
	<span class="kw">if</span>( timeline.getIsShowing() ){
        ofSetColor(<span class="dv">255</span>, <span class="dv">255</span>, <span class="dv">255</span>);
        
        <span class="co">//timeline</span>
        timeline.draw();
        
        string modeString;
        modeString = <span class="st">"Mode is "</span>;
        
        <span class="kw">switch</span> (currentMode) {
            <span class="kw">case</span> BLANK: <span class="co">//blank mode</span>
                modeString += <span class="st">"BLANK"</span>;
                <span class="kw">break</span>;</code></pre>
<p>edited for sanity.</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp">        }
        
        ofSetColor(ofColor::red);
        ofDrawBitmapString(modeString,<span class="dv">20</span>,<span class="dv">100</span>);
	}
}</code></pre>
<h2 id="appendix-2-modes-with-screen-grabs-and-code-explanation">Appendix 2: Modes, with screen grabs and code explanation</h2>
<h4 id="blank">BLANK</h4>
<div class="figure">
<div style="clear:both"><a href="..\images\project_joel/images/grabs/00_BLANK.jpg" target="_blank"><img alt="BLANK Mode" src="..\images\project_joel/images/grabs/00_BLANK.jpg" title="BLANK Mode"/></a></div><span class="caption">BLANK Mode</span>
</div>
<p>Blank mode simply displayed a blank screen. A useful default for measuring idle performance.</p>
<p>Mode update:</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp">        <span class="kw">case</span> BLANK: <span class="co">//image drawing mode</span>
            <span class="kw">break</span>;</code></pre>
<h4 id="gui">GUI</h4>
<div class="figure">
<div style="clear:both"><a href="..\images\project_joel/images/grabs/01_GUI.jpg" target="_blank"><img alt="GUI Mode" src="..\images\project_joel/images/grabs/01_GUI.jpg" title="GUI Mode"/></a></div><span class="caption">GUI Mode</span>
</div>
<p>GUI displayed several program variables and image previews of various stages of Kinect image and blob outline processing.</p>
<p>Mode update:</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp">        <span class="kw">case</span> GUI: <span class="co">//GUI MODE</span>
            <span class="kw">break</span>;</code></pre>
<p>Mode draw:</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp">        <span class="kw">case</span> GUI: <span class="co">//image drawing mode</span>
        {
            ofFill();
            ofSetColor(<span class="dv">0</span>);
            ofRect(<span class="dv">0</span>,<span class="dv">0</span>,ofGetWidth(),ofGetHeight()); <span class="co">//draw a black rectangle</span>
            
            <span class="dt">int</span> imageOffSet = <span class="dv">10</span>;
            <span class="dt">int</span> imageWidth = <span class="dv">320</span>;
            <span class="dt">int</span> imageHeight = <span class="dv">240</span>;
            <span class="dt">int</span> imageX = imageOffSet;
            
            <span class="co">// draw everything</span>
            ofSetColor(ofColor::white);
            ofEnableAlphaBlending();
            flowSolver.drawColored(imageWidth, imageHeight, <span class="dv">10</span>, <span class="dv">3</span>);
            ofDisableAlphaBlending();
            ofSetColor(ofColor::royalBlue);
            ofDrawBitmapString(<span class="st">"Flow"</span>, imageX, imageOffSet);
            ofSetColor(ofColor::white);
            colorImageRGB.draw(imageX, imageHeight+imageOffSet, imageWidth, imageHeight);
            <span class="co">//greyIRSingleChannel.draw(imageX, imageHeight+imageOffSet, imageWidth, imageHeight);</span>
            ofSetColor(ofColor::royalBlue);
            ofDrawBitmapString(<span class="st">"Kinect Video"</span>, imageX, imageHeight+imageOffSet);
            imageX += imageOffSet+imageWidth;
            ofSetColor(ofColor::white);
            kinect.drawDepth(imageX, imageHeight+imageOffSet, imageWidth, imageHeight);
            ofSetColor(ofColor::royalBlue);
            ofDrawBitmapString(<span class="st">"Kinect"</span>, imageX, imageHeight+imageOffSet);
            imageX += imageOffSet+imageWidth;
            ofSetColor(ofColor::white);
            maskImage.draw(imageX,imageHeight+imageOffSet, imageWidth, imageHeight);
            ofSetColor(ofColor::royalBlue);
            ofDrawBitmapString(<span class="st">"Mask"</span>, imageX, imageHeight+imageOffSet);
            imageX = imageOffSet;
            ofSetColor(ofColor::white);
            depthOrig.draw(imageX,imageHeight+imageOffSet+imageHeight+imageOffSet, imageWidth, imageHeight);
            ofSetColor(ofColor::royalBlue);
            ofDrawBitmapString(<span class="st">"Original Depth"</span>, imageX, imageHeight+imageOffSet+imageHeight+imageOffSet);
            imageX += imageOffSet+imageWidth;
            ofSetColor(ofColor::white);
            depthProcessed.draw(imageX,imageHeight+imageOffSet+imageHeight+imageOffSet, imageWidth, imageHeight);
            ofSetColor(ofColor::royalBlue);
            ofDrawBitmapString(<span class="st">"Depth Processed"</span>, imageX, imageHeight+imageOffSet+imageHeight+imageOffSet);
            imageX += imageOffSet+imageWidth;
            ofSetColor(ofColor::white);
            depthContours.draw(imageX, imageHeight+imageOffSet+imageHeight+imageOffSet, imageWidth, imageHeight);
            ofSetColor(ofColor::royalBlue);
            ofDrawBitmapString(<span class="st">"Depth Contours"</span>, imageX, imageHeight+imageOffSet+imageHeight+imageOffSet);
            ofSetColor(ofColor::skyBlue);
            <span class="co">// draw instructions</span>
            stringstream reportStream;
            reportStream
            &lt;&lt; <span class="st">"f to fullscreen, g to show/hide timeline, m to show/hide mouse"</span> &lt;&lt; endl
            &lt;&lt; <span class="st">"a/s to cycle through scenes"</span> &lt;&lt; endl
            &lt;&lt; <span class="st">"Function	                                      Shortcut"</span> &lt;&lt; endl
            &lt;&lt; <span class="st">"Cut Selection	                                  command+x"</span> &lt;&lt; endl
            &lt;&lt; <span class="st">"Copy Selection	                                  command+c"</span> &lt;&lt; endl
            &lt;&lt; <span class="st">"Paste Selection	                                  command+v"</span> &lt;&lt; endl
            &lt;&lt; <span class="st">"Undo	                                          command+z"</span> &lt;&lt; endl
            &lt;&lt; <span class="st">"Redo	                                          shift+command+z"</span> &lt;&lt; endl
            &lt;&lt; <span class="st">"Select all keyframes in Focused track	          command+a"</span> &lt;&lt; endl
            &lt;&lt; <span class="st">"Add all keyframes in Focused track to selection   command+shift+a"</span> &lt;&lt; endl
            &lt;&lt; <span class="st">"Delete all selected keyframes	                  delete or backspace"</span> &lt;&lt; endl
            &lt;&lt; <span class="st">"Nudge keyframes a little	                      arrow keys"</span> &lt;&lt; endl
            &lt;&lt; <span class="st">"Nudge keyframes a little more	                  shift+arrow keys"</span> &lt;&lt; endl
            &lt;&lt; <span class="st">"Expand Focused track	                          alt+e"</span> &lt;&lt; endl
            &lt;&lt; <span class="st">"Collapse all tracks	                              alt+c"</span> &lt;&lt; endl
            &lt;&lt; <span class="st">"Evenly distribute track sizes	                  alt+shift+c"</span> &lt;&lt; endl
            &lt;&lt; <span class="st">", fps: "</span> &lt;&lt; ofGetFrameRate() &lt;&lt; endl
            &lt;&lt; <span class="st">"press shift squerty 1-5 &amp; 0 to change the led mode"</span> &lt;&lt; endl;
            ofDrawBitmapString(reportStream.str(),<span class="dv">20</span>,ofGetHeight()/<span class="fl">2.f</span>);
            
            stringstream m;
            m &lt;&lt; <span class="st">"fps "</span> &lt;&lt; ofGetFrameRate() &lt;&lt; endl
            &lt;&lt; <span class="st">"pyramid scale: "</span> &lt;&lt; flowSolver.getPyramidScale() &lt;&lt; <span class="st">" p/P"</span> &lt;&lt; endl
            &lt;&lt; <span class="st">"pyramid levels: "</span> &lt;&lt; flowSolver.getPyramidLevels() &lt;&lt; <span class="st">" l/L"</span> &lt;&lt; endl
            &lt;&lt; <span class="st">"averaging window size: "</span> &lt;&lt; flowSolver.getWindowSize() &lt;&lt; <span class="st">" w/W"</span> &lt;&lt; endl
            &lt;&lt; <span class="st">"iterations per level: "</span> &lt;&lt; flowSolver.getIterationsPerLevel() &lt;&lt; <span class="st">" i/I"</span> &lt;&lt; endl
            &lt;&lt; <span class="st">"expansion area: "</span> &lt;&lt; flowSolver.getExpansionArea() &lt;&lt; <span class="st">" a/A"</span> &lt;&lt; endl
            &lt;&lt; <span class="st">"expansion sigma: "</span> &lt;&lt; flowSolver.getExpansionSigma() &lt;&lt; <span class="st">" s/S"</span> &lt;&lt; endl
            &lt;&lt; <span class="st">"flow feedback: "</span> &lt;&lt; flowSolver.getFlowFeedback() &lt;&lt; <span class="st">" f/F"</span> &lt;&lt; endl
            &lt;&lt; <span class="st">"gaussian filtering: "</span> &lt;&lt; flowSolver.getGaussianFiltering() &lt;&lt; <span class="st">" g/G"</span>;
            
            ofDrawBitmapString(m.str(), <span class="dv">20+320</span>, <span class="dv">20</span>);
        }
            <span class="kw">break</span>;</code></pre>
<h4 id="video">VIDEO</h4>
<div class="figure">
<div style="clear:both"><a href="..\images\project_joel/images/grabs/02_VIDEO.jpg" target="_blank"><img alt="VIDEO Mode" src="..\images\project_joel/images/grabs/02_VIDEO.jpg" title="VIDEO Mode"/></a></div><span class="caption">VIDEO Mode</span>
</div>
<p>Video mode displayed the current frame of the unprocessed video file.</p>
<p>Mode update:</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp">        <span class="kw">case</span> VIDEO:
            <span class="kw">break</span>;</code></pre>
<p>Mode draw:</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp">        <span class="kw">case</span> VIDEO: <span class="co">//the film</span>
            ofFill();
            ofSetColor(<span class="dv">255</span>);
            timeline.getVideoPlayer(<span class="st">"video"</span>)-&gt;draw(<span class="dv">0</span>, <span class="dv">0</span>, ofGetWidth(),ofGetHeight());
            <span class="kw">break</span>;</code></pre>
<h4 id="videocircles">VIDEOCIRCLES</h4>
<div class="figure">
<div style="clear:both"><a href="..\images\project_joel/images/grabs/03_VIDEOCIRCLES.jpg" target="_blank"><img alt="VIDEOCIRCLES Mode" src="..\images\project_joel/images/grabs/03_VIDEOCIRCLES.jpg" title="VIDEOCIRCLES Mode"/></a></div><span class="caption">VIDEOCIRCLES Mode</span>
</div>
<p>VideoCircles was a direct cut and paste from the <code>examples/video/osxHighPerformanceVideoPlayerExample</code>. This code was useful during initial development to discover the performance hit for individual pixel array access. A lot of my early development during projects is based around finding what the limits of various prospective coding functionality is - getting to a happy mix of performance and functionality.</p>
<p>Mode update:</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp">        <span class="kw">case</span> VIDEOCIRCLES: <span class="co">//the film as circles</span>
            <span class="kw">break</span>;</code></pre>
<p>Mode draw:</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp">        <span class="kw">case</span> VIDEOCIRCLES: <span class="co">//the film as circles</span>
            {
                ofFill();
                ofSetColor(<span class="dv">0</span>);
                ofRect(<span class="dv">0</span>,<span class="dv">0</span>,ofGetWidth(),ofGetHeight()); <span class="co">//draw a black rectangle</span>
                <span class="kw">if</span> (timeline.getVideoPlayer(<span class="st">"video"</span>)-&gt;isLoaded()) {
                    <span class="dt">unsigned</span> <span class="dt">char</span> * pixels = timeline.getVideoPlayer(<span class="st">"video"</span>)-&gt;getPixels();
                    ofPixelsRef pixelsRef = timeline.getVideoPlayer(<span class="st">"video"</span>)-&gt;getPixelsRef();
                    
                    <span class="co">// let's move through the "RGB(A)" char array</span>
                    <span class="co">// using the red pixel to control the size of a circle.</span>
                    <span class="co">//ofSetColor(timeline.getColor("colour"));</span>
                    ofSetColor(ofColor::lightBlue);
                    
                    <span class="dt">float</span> circleSpacing = <span class="fl">10.f</span>;
                    
                    <span class="dt">float</span> widthRatio = ofGetWidth()/timeline.getVideoPlayer(<span class="st">"video"</span>)-&gt;getWidth();
                    <span class="dt">float</span> heightRatio = ofGetHeight()/timeline.getVideoPlayer(<span class="st">"video"</span>)-&gt;getHeight();
                    
                    <span class="kw">for</span>(<span class="dt">int</span> i = <span class="dv">0</span>; i &lt; timeline.getVideoPlayer(<span class="st">"video"</span>)-&gt;getWidth(); i+= <span class="dv">8</span>){
                        <span class="kw">for</span>(<span class="dt">int</span> j = <span class="dv">0</span>; j &lt; timeline.getVideoPlayer(<span class="st">"video"</span>)-&gt;getHeight(); j+= <span class="dv">8</span>){
                            ofColor pixelColor = timeline.getVideoPlayer(<span class="st">"video"</span>)-&gt;getPixelsRef().getColor(i, j);
                            <span class="dt">int</span> b = pixelColor.b;
                            <span class="dt">float</span> val = <span class="dv">1</span> - ((<span class="dt">float</span>)b / <span class="fl">255.</span><span class="er">0f</span>); <span class="co">//more blue in the arctic!</span>
                            ofCircle(i*widthRatio, j*heightRatio, circleSpacing * val);
                        }
                    }
                }
            }
            <span class="kw">break</span>;</code></pre>
<h4 id="kinectpointcloud">KINECTPOINTCLOUD</h4>
<div class="figure">
<div style="clear:both"><a href="..\images\project_joel/images/grabs/04_KINECTPOINTCLOUD.jpg" target="_blank"><img alt="KINECTPOINTCLOUD Mode" src="..\images\project_joel/images/grabs/04_KINECTPOINTCLOUD.jpg" title="KINECTPOINTCLOUD Mode"/></a></div><span class="caption">KINECTPOINTCLOUD Mode</span>
</div>
<p>Another cut and paste from addon example code, this time from the now core <code>ofxKinect</code>.</p>
<p>Mode update:</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp">        <span class="kw">case</span> KINECTPOINTCLOUD: <span class="co">//draw the kinect camera depth cloud</span>
            <span class="kw">break</span>;</code></pre>
<p>Mode draw:</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp">        <span class="kw">case</span> KINECTPOINTCLOUD: <span class="co">//draw the kinect camera depth cloud</span>
            easyCam.begin();
            drawPointCloud();
            easyCam.end();
            <span class="kw">break</span>;</code></pre>
<h4 id="slitscanbasic">SLITSCANBASIC</h4>
<div class="figure">
<div style="clear:both"><a href="..\images\project_joel/images/grabs/08_SLITSCANBASIC.jpg" target="_blank"><img alt="SLITSCANBASIC Mode" src="..\images\project_joel/images/grabs/08_SLITSCANBASIC.jpg" title="SLITSCANBASIC Mode"/></a></div><span class="caption">SLITSCANBASIC Mode</span>
</div>
<p>The most basic of the slitscan modes on this project - a direct port of example functionality in ofxSlitscan - but with the possibility of changing the slitscan PNG source file on the ofxTimeline GUI.</p>
<p>Mode update:</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp">        <span class="kw">case</span> SLITSCANBASIC: <span class="co">//slit scan the movie on the grey from the kinect depth grey</span>
        {
            <span class="co">//check slit scan...</span>
            <span class="dt">int</span> theCurrentSlitScan = timeline.getValue(<span class="st">"slitscan"</span>);
            <span class="kw">if</span>(prevSlitScan != theCurrentSlitScan){
                slitScanSliderSlid(); <span class="co">//only update when you have to...</span>
                prevSlitScan = theCurrentSlitScan;
            }
            
            <span class="kw">if</span>(timeline.getVideoPlayer(<span class="st">"video"</span>)-&gt;isFrameNew()){
                slitScan.addImage(timeline.getVideoPlayer(<span class="st">"video"</span>)-&gt;getPixelsRef());
            }
        }
            <span class="kw">break</span>;</code></pre>
<p>Mode draw:</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp">        <span class="kw">case</span> SLITSCANBASIC: <span class="co">//slit scan the movie on depth png</span>
            slitScan.getOutputImage().draw(<span class="dv">0</span>, <span class="dv">0</span>, ofGetWidth(), ofGetHeight());
            
            <span class="co">//white fur</span>
            ofEnableAlphaBlending();
            flowSolver.drawGrey(ofGetWidth(),ofGetHeight(), <span class="dv">10</span>, <span class="dv">3</span>);
            ofDisableAlphaBlending();
            
            <span class="kw">break</span>;</code></pre>
<h4 id="slitscankinectdepthgrey">SLITSCANKINECTDEPTHGREY</h4>
<div class="figure">
<div style="clear:both"><a href="..\images\project_joel/images/grabs/09_SLITSCANKINECTDEPTHGREY.jpg" target="_blank"><img alt="SLITSCANKINECTDEPTHGREY Mode" src="..\images\project_joel/images/grabs/09_SLITSCANKINECTDEPTHGREY.jpg" title="SLITSCANKINECTDEPTHGREY Mode"/></a></div><span class="caption">SLITSCANKINECTDEPTHGREY Mode</span>
</div>
<p>The most basic of novel slitscan modes developed for this project - feeding the Kinect depth image into ofxSlitscan on a per frame basis - once I realised this would still result in interactive frame rates I knew the project would succeed.</p>
<p>Mode update:</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp">        <span class="kw">case</span> SLITSCANKINECTDEPTHGREY: <span class="co">//slit scan the movie on the grey from the kinect depth grey</span>
        {
            <span class="kw">if</span>(timeline.getVideoPlayer(<span class="st">"video"</span>)-&gt;isFrameNew()){
                <span class="co">//kinect slitscan</span>
                <span class="co">//depthPixels.setFromPixels(kinect.getDepthPixelsRef());</span>
                depthPixels.setFromPixels(depthProcessed.getPixelsRef());
                depthPixels.resize(timeline.getVideoPlayer(<span class="st">"video"</span>)-&gt;getWidth(), timeline.getVideoPlayer(<span class="st">"video"</span>)-&gt;getHeight());
                <span class="co">//            slitScanDepthGrey.setDelayMap(depthPixels);</span>
                <span class="co">//            slitScanDepthGrey.addImage(timeline.getVideoPlayer("video")-&gt;getPixelsRef());</span>
                slitScan.setDelayMap(depthPixels);
                slitScan.addImage(timeline.getVideoPlayer(<span class="st">"video"</span>)-&gt;getPixelsRef());
            }
        }
            <span class="kw">break</span>;</code></pre>
<p>Mode draw:</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp">        <span class="kw">case</span> SLITSCANKINECTDEPTHGREY: <span class="co">//slit scan the movie on the grey from the kinect depth grey</span>
            slitScan.getOutputImage().draw(<span class="dv">0</span>, <span class="dv">0</span>, ofGetWidth(), ofGetHeight());
            <span class="co">//slitScanDepthGrey.getOutputImage().draw(0, 0, ofGetWidth(), ofGetHeight());</span>
            <span class="kw">break</span>;</code></pre>
<h4 id="sparkle">SPARKLE</h4>
<div class="figure">
<div style="clear:both"><a href="..\images\project_joel/images/grabs/07_SPARKLE.jpg" target="_blank"><img alt="SPARKLE Mode" src="..\images\project_joel/images/grabs/07_SPARKLE.jpg" title="SPARKLE Mode"/></a></div><span class="caption">SPARKLE Mode</span>
</div>
<p>An experiment with using previously developed Somantics functionality into ofxTimeline.</p>
<p>Mode update:</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp">        <span class="kw">case</span> SPARKLE: <span class="co">//sparkles on the slitscan</span>
        {
            <span class="co">//update the sparkles come what may...</span>
            someSparkles.update(&amp;depthContours);
            someSparkles.draw(ofColor::white);
            <span class="co">//someSparkles.draw(timeline.getColor("colour"));</span>
            
            ofImage distortionMap;
            distortionMap.allocate(someSparkles.theFBO.getWidth(), someSparkles.theFBO.getHeight(), OF_IMAGE_COLOR);
            
            someSparkles.theFBO.readToPixels(distortionMap.getPixelsRef());
            
            distortionMap.resize(timeline.getVideoPlayer(<span class="st">"video"</span>)-&gt;getWidth(), timeline.getVideoPlayer(<span class="st">"video"</span>)-&gt;getHeight());
            slitScan.setDelayMap(distortionMap);
            
            <span class="kw">if</span>(timeline.getVideoPlayer(<span class="st">"video"</span>)-&gt;isFrameNew()){
                slitScan.addImage(timeline.getVideoPlayer(<span class="st">"video"</span>)-&gt;getPixelsRef());
            }
        }
            <span class="kw">break</span>;</code></pre>
<p>Mode draw:</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp">        <span class="kw">case</span> SPARKLE:
            <span class="co">//do some sparkles - used the slit scan to hold it....</span>
            slitScan.getOutputImage().draw(<span class="dv">0</span>, <span class="dv">0</span>, ofGetWidth(), ofGetHeight());
            <span class="co">//ofSetColor(255,255,255);</span>
            <span class="co">//someSparkles.theFBO.draw(0, 0, ofGetWidth(), ofGetHeight());</span>
            <span class="kw">break</span>;</code></pre>
<h4 id="verticalmirror">VERTICALMIRROR</h4>
<div class="figure">
<div style="clear:both"><a href="..\images\project_joel/images/grabs/05_VERTICALMIRROR.jpg" target="_blank"><img alt="VERTICALMIRROR Mode" src="..\images\project_joel/images/grabs/05_VERTICALMIRROR.jpg" title="VERTICALMIRROR Mode"/></a></div><span class="caption">VERTICALMIRROR Mode</span>
</div>
<p>A vertical mirror on the video playback - again ported directly from Somantics.</p>
<p>Mode update:</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp">        <span class="kw">case</span> VERTICALMIRROR: <span class="co">//vertical mirror</span>
        {
            <span class="kw">if</span>(timeline.getVideoPlayer(<span class="st">"video"</span>)-&gt;isFrameNew()){
                verticalMirrorImage.setFromPixels(timeline.getVideoPlayer(<span class="st">"video"</span>)-&gt;getPixels(), verticalMirrorImage.getWidth(), verticalMirrorImage.getHeight());
                
                verticalMirrorImage.updateTexture();
            }
        }
            <span class="kw">break</span>;</code></pre>
<p>Mode draw:</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp">        <span class="kw">case</span> VERTICALMIRROR:
        {
            <span class="dt">bool</span> usingNormTexCoords = ofGetUsingNormalizedTexCoords();
            
            <span class="kw">if</span>(!usingNormTexCoords) {
                ofEnableNormalizedTexCoords();
            }
            
            verticalMirrorImage.getTextureReference().bind();
            
            ofMesh mesh;
            mesh.clear();
            mesh.addVertex(ofVec3f(<span class="dv">0</span>, <span class="dv">0</span>));
            mesh.addVertex(ofVec3f(<span class="dv">0</span>, ofGetHeight()));
            mesh.addVertex(ofVec3f(ofGetWidth()/<span class="dv">2</span>, <span class="dv">0</span>));
            mesh.addVertex(ofVec3f(ofGetWidth()/<span class="dv">2</span>, ofGetHeight()));
            mesh.addVertex(ofVec3f(ofGetWidth(), <span class="dv">0</span>));
            mesh.addVertex(ofVec3f(ofGetWidth(), ofGetHeight()));
            
            
            mesh.addTexCoord(ofVec2f(<span class="fl">0.</span><span class="dv">25</span>, <span class="fl">0.</span><span class="dv">0</span>));
            mesh.addTexCoord(ofVec2f(<span class="fl">0.</span><span class="dv">25</span>, <span class="fl">1.</span><span class="dv">0</span>));
            mesh.addTexCoord(ofVec2f(<span class="fl">0.</span><span class="dv">75</span>, <span class="fl">0.</span><span class="dv">0</span>));
            mesh.addTexCoord(ofVec2f(<span class="fl">0.</span><span class="dv">75</span>, <span class="fl">1.</span><span class="dv">0</span>));
            mesh.addTexCoord(ofVec2f(<span class="fl">0.</span><span class="dv">25</span>, <span class="fl">0.</span><span class="dv">0</span>));
            mesh.addTexCoord(ofVec2f(<span class="fl">0.</span><span class="dv">25</span>, <span class="fl">1.</span><span class="dv">0</span>));
            
            mesh.setMode(OF_PRIMITIVE_TRIANGLE_STRIP);
            ofSetColor(ofColor::white);
            mesh.draw();
            
            verticalMirrorImage.getTextureReference().unbind();
            
            <span class="co">// pop normalized tex coords</span>
            <span class="kw">if</span>(!usingNormTexCoords) {
                ofDisableNormalizedTexCoords();
            }
            
            <span class="co">//white fur</span>
            ofEnableAlphaBlending();
            flowSolver.drawGrey(ofGetWidth(),ofGetHeight(), <span class="dv">10</span>, <span class="dv">3</span>);
            ofDisableAlphaBlending();
        }
            <span class="kw">break</span>;</code></pre>
<h4 id="horizontalmirror">HORIZONTALMIRROR</h4>
<div class="figure">
<div style="clear:both"><a href="..\images\project_joel/images/grabs/06_HORIZONTAL.jpg" target="_blank"><img alt="HORIZONTALMIRROR Mode" src="..\images\project_joel/images/grabs/06_HORIZONTAL.jpg" title="HORIZONTALMIRROR Mode"/></a></div><span class="caption">HORIZONTALMIRROR Mode</span>
</div>
<p>A horizontal mirror on the video playback - again ported directly from Somantics.</p>
<p>Mode update:</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp">        <span class="kw">case</span> HORIZONTALMIRROR: <span class="co">//HORIZONTALMIRROR mirror</span>
        {
            <span class="kw">if</span>(timeline.getVideoPlayer(<span class="st">"video"</span>)-&gt;isFrameNew()){
                horizontalMirrorImage.setFromPixels(timeline.getVideoPlayer(<span class="st">"video"</span>)-&gt;getPixels(), horizontalMirrorImage.getWidth(), horizontalMirrorImage.getHeight());
                
                horizontalMirrorImage.updateTexture();
            }
        }
            <span class="kw">break</span>;</code></pre>
<p>Mode draw:</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp">        <span class="kw">case</span> HORIZONTALMIRROR:
        {
            <span class="dt">bool</span> usingNormTexCoords = ofGetUsingNormalizedTexCoords();
            
            <span class="kw">if</span>(!usingNormTexCoords) {
                ofEnableNormalizedTexCoords();
            }
            
            horizontalMirrorImage.getTextureReference().bind();
            
            ofMesh mesh;
            mesh.clear();
            mesh.addVertex(ofVec3f(ofGetWidth(), <span class="dv">0</span>));
            mesh.addVertex(ofVec3f(<span class="dv">0</span>, <span class="dv">0</span>));
            mesh.addVertex(ofVec3f(ofGetWidth(), ofGetHeight()/<span class="dv">2</span>));
            mesh.addVertex(ofVec3f(<span class="dv">0</span>, ofGetHeight()/<span class="dv">2</span>));
            mesh.addVertex(ofVec3f(ofGetWidth(), ofGetHeight()));
            mesh.addVertex(ofVec3f(<span class="dv">0</span>,ofGetHeight()));
            
            mesh.addTexCoord(ofVec2f(<span class="fl">1.</span><span class="dv">0</span>, <span class="fl">0.</span><span class="dv">25</span>));
            mesh.addTexCoord(ofVec2f(<span class="fl">0.</span><span class="dv">0</span>, <span class="fl">0.</span><span class="dv">25</span>));
            mesh.addTexCoord(ofVec2f(<span class="fl">1.</span><span class="dv">0</span>, <span class="fl">0.</span><span class="dv">75</span>));
            mesh.addTexCoord(ofVec2f(<span class="fl">0.</span><span class="dv">0</span>, <span class="fl">0.</span><span class="dv">75</span>));
            mesh.addTexCoord(ofVec2f(<span class="fl">1.</span><span class="dv">0</span>, <span class="fl">0.</span><span class="dv">25</span>));
            mesh.addTexCoord(ofVec2f(<span class="fl">0.</span><span class="dv">0</span>, <span class="fl">0.</span><span class="dv">25</span>));
            
            mesh.setMode(OF_PRIMITIVE_TRIANGLE_STRIP);
            ofSetColor(ofColor::white);
            mesh.draw();
            
            horizontalMirrorImage.getTextureReference().unbind();
            
            <span class="co">// pop normalized tex coords</span>
            <span class="kw">if</span>(!usingNormTexCoords) {
                ofDisableNormalizedTexCoords();
            }
            
            <span class="co">//white fur</span>
            ofEnableAlphaBlending();
            flowSolver.drawGrey(ofGetWidth(),ofGetHeight(), <span class="dv">10</span>, <span class="dv">3</span>);
            ofDisableAlphaBlending();
        }
            <span class="kw">break</span>;</code></pre>
<h4 id="kaleidoscope">KALEIDOSCOPE</h4>
<div class="figure">
<div style="clear:both"><a href="..\images\project_joel/images/grabs/10_KALEIDOSCOPE.jpg" target="_blank"><img alt="KALEIDOSCOPE Mode" src="..\images\project_joel/images/grabs/10_KALEIDOSCOPE.jpg" title="KALEIDOSCOPE Mode"/></a></div><span class="caption">KALEIDOSCOPE Mode</span>
</div>
<p>A Kaleidoscope mirror on the video playback - again ported directly from Somantics, using <a href="http://mazbox.com/" target="_blank">Marek Bereza's</a> logic.</p>
<p>Mode update:</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp">        <span class="kw">case</span> KALEIDOSCOPE: <span class="co">//kaleidsocope</span>
        {
            <span class="kw">if</span>(timeline.getVideoPlayer(<span class="st">"video"</span>)-&gt;isFrameNew()){
                kaleidoscopeMirrorImage.setFromPixels(timeline.getVideoPlayer(<span class="st">"video"</span>)-&gt;getPixels(), kaleidoscopeMirrorImage.getWidth(), kaleidoscopeMirrorImage.getHeight());
                
                kaleidoscopeMirrorImage.updateTexture();
            }
        }
            <span class="kw">break</span>;</code></pre>
<p>Mode draw:</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp">        <span class="kw">case</span> KALEIDOSCOPE:
        {
            <span class="dt">bool</span> usingNormTexCoords = ofGetUsingNormalizedTexCoords();
            
            <span class="kw">if</span>(!usingNormTexCoords) {
                ofEnableNormalizedTexCoords();
            }
            
            kaleidoscopeMirrorImage.getTextureReference().bind();
            
            <span class="dt">int</span> star = ((<span class="dt">int</span>)timeline.getValue(<span class="st">"star"</span>)*<span class="dv">2</span>);<span class="co">//8; //get star from the timeline gui, but multiply by 2 to get to always even</span>
            <span class="dt">float</span> offset = timeline.getValue(<span class="st">"offset"</span>);<span class="co">//0.5f; // get offset from the timeline gui</span>
            <span class="dt">float</span> angle = <span class="fl">360.f</span>/star; <span class="co">//8 sides to start</span>
            
            
            
			ofMesh mesh;
            
			ofVec3f vec(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>);
			mesh.addVertex(vec);
			vec.x += ofGetHeight()/<span class="dv">2</span>;
            
			<span class="kw">for</span>(<span class="dt">int</span> i = <span class="dv">0</span>; i &lt; star; i++) {
				mesh.addVertex(vec);
				vec.rotate(angle, ofVec3f(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>));
			}
            
			<span class="co">// close the loop</span>
			mesh.addVertex(vec);
            
            
            
			<span class="co">// now work out the texcoords</span>
			<span class="co">/*</span>
<span class="co">			 __________________</span>
<span class="co">			 |   \        /   |</span>
<span class="co">			 |    \      /    |</span>
<span class="co">			 |     \    /     |</span>
<span class="co">			 |      \  /      |</span>
<span class="co">			 |       \/       |</span>
<span class="co">			 +----------------+</span>
<span class="co">             </span>
<span class="co">			 A v shape out of the centre of the camera texture</span>
<span class="co">			 */</span>
            
            
            
			<span class="dt">float</span> realOffset = <span class="fl">0.</span><span class="dv">5</span>;
			<span class="co">// normalized distance from the centre (half the width of the above 'V')</span>
			<span class="dt">float</span> dist = ABS((<span class="dt">float</span>)kaleidoscopeMirrorImage.getHeight()*tan(ofDegToRad(angle)*<span class="fl">0.</span><span class="dv">5</span>))/(<span class="dt">float</span>)kaleidoscopeMirrorImage.getHeight();
            
            
			<span class="co">// the realOffset is where the (normalized) middle of the 'V' is on the x-axis</span>
			realOffset = ofMap(offset, <span class="dv">0</span>, <span class="dv">1</span>, dist, <span class="dv">1</span>-dist);
            
            
			<span class="co">// this is the point at the bottom of the triangle - our centre for the triangle fan</span>
			mesh.addTexCoord(ofVec2f(realOffset, <span class="dv">1</span>));
            
            
			ofVec2f ta(realOffset-dist, <span class="dv">0</span>);
			ofVec2f tb(realOffset+dist, <span class="dv">0</span>);
			<span class="kw">for</span>(<span class="dt">int</span> i = <span class="dv">0</span>; i &lt;= star; i++) {
				<span class="kw">if</span>(i%<span class="dv">2</span>==<span class="dv">0</span>) {
					mesh.addTexCoord(ta);
				} <span class="kw">else</span> {
					mesh.addTexCoord(tb);
				}
			}
            
            
			glPushMatrix();
			glTranslatef(ofGetWidth()/<span class="dv">2</span>, ofGetHeight()/<span class="dv">2</span>, <span class="dv">0</span>);
			mesh.setMode(OF_PRIMITIVE_TRIANGLE_FAN);
			mesh.draw();
			glPopMatrix();
            
            kaleidoscopeMirrorImage.getTextureReference().unbind();
            
            <span class="co">// pop normalized tex coords</span>
            <span class="kw">if</span>(!usingNormTexCoords) {
                ofDisableNormalizedTexCoords();
            }
            
            <span class="co">//white fur</span>
            ofEnableAlphaBlending();
            flowSolver.drawGrey(ofGetWidth(),ofGetHeight(), <span class="dv">10</span>, <span class="dv">3</span>);
            ofDisableAlphaBlending();
        }
            <span class="kw">break</span>;</code></pre>
<h4 id="colourfur">COLOURFUR</h4>
<div class="figure">
<div style="clear:both"><a href="..\images\project_joel/images/grabs/12_COLOURFUR.jpg" target="_blank"><img alt="COLOURFUR Mode" src="..\images\project_joel/images/grabs/12_COLOURFUR.jpg" title="COLOURFUR Mode"/></a></div><span class="caption">COLOURFUR Mode</span>
</div>
<p>A direct port of <a href="https://github.com/timscaffidi/ofxOpticalFlowFarneback" target="_blank">Tim Scaffidi's ofxOpticalFlowFarneback</a> demo code.</p>
<p>Mode update:</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp">        <span class="kw">case</span> COLOURFUR: <span class="co">//COLOURFUR</span>
        {
        }
            <span class="kw">break</span>;</code></pre>
<p>Mode draw:</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp">
        <span class="kw">case</span> COLOURFUR:
        {
            ofSetColor(ofColor::white);
            timeline.getVideoPlayer(<span class="st">"video"</span>)-&gt;draw(<span class="dv">0</span>, <span class="dv">0</span>, ofGetWidth(),ofGetHeight());
            ofEnableAlphaBlending();
            flowSolver.drawColored(ofGetWidth(),ofGetHeight(), <span class="dv">10</span>, <span class="dv">3</span>);
            ofDisableAlphaBlending();
        }
            <span class="kw">break</span>;</code></pre>
<h4 id="depth">DEPTH</h4>
<div class="figure">
<div style="clear:both"><a href="..\images\project_joel/images/grabs/13_DEPTH.jpg" target="_blank"><img alt="DEPTH Mode" src="..\images\project_joel/images/grabs/13_DEPTH.jpg" title="DEPTH Mode"/></a></div><span class="caption">DEPTH Mode</span>
</div>
<p>A simple mode to display the depth image directly - useful for debugging when onsite.</p>
<p>Mode update:</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp">        <span class="kw">case</span> DEPTH: <span class="co">//DEPTH</span>
        {
        }</code></pre>
<p>Mode draw:</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp">        <span class="kw">case</span> DEPTH:
        {
            depthProcessed.draw(<span class="dv">0</span>,<span class="dv">0</span>,ofGetWidth(), ofGetHeight());
        }
            <span class="kw">break</span>;</code></pre>
<h4 id="shatter">SHATTER</h4>
<div class="figure">
<div style="clear:both"><a href="..\images\project_joel/images/grabs/14_SHATTER.jpg" target="_blank"><img alt="SHATTER Mode" src="..\images\project_joel/images/grabs/14_SHATTER.jpg" title="SHATTER Mode"/></a></div><span class="caption">SHATTER Mode</span>
</div>
<p>A direct port of <a href="http://vanderlin.cc/projects/feedback/" target="_blank">Todd Vanderlin's</a> code that he wrote for the Feedback project, but using it as live delay map input to the Slitscan.</p>
<p>Mode update:</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp">        <span class="kw">case</span> SHATTER:
        {
            <span class="co">//update the shatter</span>
            theShatter.update(&amp;depthContours);
            theShatter.draw(ofColor::white);
            
            ofImage distortionMap;
            distortionMap.allocate(theShatter.theFBO.getWidth(), theShatter.theFBO.getHeight(), OF_IMAGE_COLOR);
            
            theShatter.theFBO.readToPixels(distortionMap.getPixelsRef());
            
            distortionMap.resize(timeline.getVideoPlayer(<span class="st">"video"</span>)-&gt;getWidth(), timeline.getVideoPlayer(<span class="st">"video"</span>)-&gt;getHeight());
            slitScan.setDelayMap(distortionMap);
            
            <span class="kw">if</span>(timeline.getVideoPlayer(<span class="st">"video"</span>)-&gt;isFrameNew()){
                slitScan.addImage(timeline.getVideoPlayer(<span class="st">"video"</span>)-&gt;getPixelsRef());
            }
        }
            <span class="kw">break</span>;
</code></pre>
<p>Mode draw:</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp">        <span class="kw">case</span> SHATTER:
        {
            <span class="co">//do some shattering - used the slit scan to hold it....</span>
            slitScan.getOutputImage().draw(<span class="dv">0</span>, <span class="dv">0</span>, ofGetWidth(), ofGetHeight());
            <span class="co">//ofSetColor(255,255,255);</span>
            <span class="co">//theShatter.theFBO.draw(0, 0, ofGetWidth(), ofGetHeight());</span>
        }
            <span class="kw">break</span>;</code></pre>
<h4 id="selfslitscan">SELFSLITSCAN</h4>
<div class="figure">
<div style="clear:both"><a href="..\images\project_joel/images/grabs/15_SELFSLITSCAN.jpg" target="_blank"><img alt="SELFSLITSCAN Mode" src="..\images\project_joel/images/grabs/15_SELFSLITSCAN.jpg" title="SELFSLITSCAN Mode"/></a></div><span class="caption">SELFSLITSCAN Mode</span>
</div>
<p>Feeding the greyscale image of the current film frame back into the SlitScan delay map made for some interesting feedback effects.</p>
<p>Mode update:</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp">        <span class="kw">case</span> SELFSLITSCAN:
        {
            <span class="kw">if</span>(timeline.getVideoPlayer(<span class="st">"video"</span>)-&gt;isFrameNew()){
                <span class="co">//self slitscan</span>
<span class="co">//                ofImage selfSlitScanDelayMap;</span>
<span class="co">//                selfSlitScanDelayMap.allocate(timeline.getVideoPlayer("video")-&gt;getWidth(), timeline.getVideoPlayer("video")-&gt;getHeight(), OF_IMAGE_COLOR);</span>
<span class="co">//                selfSlitScanDelayMap.setFromPixels(timeline.getVideoPlayer("video")-&gt;getPixelsRef());</span>
               
                slitScan.setDelayMap(timeline.getVideoPlayer(<span class="st">"video"</span>)-&gt;getPixelsRef());
                slitScan.addImage(timeline.getVideoPlayer(<span class="st">"video"</span>)-&gt;getPixelsRef());
            }
        }
            <span class="kw">break</span>;</code></pre>
<p>Mode draw:</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp">        <span class="kw">case</span> SELFSLITSCAN:
        {
            <span class="co">//do some SELFSLITSCAN - used the slit scan to hold it....</span>
            ofSetColor(<span class="dv">255</span>,<span class="dv">255</span>,<span class="dv">255</span>);
            slitScan.getOutputImage().draw(<span class="dv">0</span>, <span class="dv">0</span>, ofGetWidth(), ofGetHeight());
        }
            <span class="kw">break</span>;</code></pre>
<h4 id="spikyblobslitscan">SPIKYBLOBSLITSCAN</h4>
<div class="figure">
<div style="clear:both"><a href="..\images\project_joel/images/grabs/16_SPIKYBLOBSLITSCAN.jpg" target="_blank"><img alt="SPIKYBLOBSLITSCAN Mode" src="..\images\project_joel/images/grabs/16_SPIKYBLOBSLITSCAN.jpg" title="SPIKYBLOBSLITSCAN Mode"/></a></div><span class="caption">SPIKYBLOBSLITSCAN Mode</span>
</div>
<p>Feeding the Spiked blob outline back into the SlitScan delay map.</p>
<p>Mode update:</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp">        <span class="kw">case</span> SPIKYBLOBSLITSCAN:
        {
            <span class="co">//SPIKYBLOBSLITSCAN</span>
            <span class="co">//update the spikes come what may...</span>
            theSpikey.update(&amp;depthContours);
            theSpikey.draw(ofColor::white);
            
            ofImage distortionMap;
            distortionMap.allocate(theSpikey.theFBO.getWidth(), theSpikey.theFBO.getHeight(), OF_IMAGE_COLOR);
            
            theSpikey.theFBO.readToPixels(distortionMap.getPixelsRef());
            
            distortionMap.resize(timeline.getVideoPlayer(<span class="st">"video"</span>)-&gt;getWidth(), timeline.getVideoPlayer(<span class="st">"video"</span>)-&gt;getHeight());
            slitScan.setDelayMap(distortionMap);
            
            <span class="kw">if</span>(timeline.getVideoPlayer(<span class="st">"video"</span>)-&gt;isFrameNew()){
                slitScan.addImage(timeline.getVideoPlayer(<span class="st">"video"</span>)-&gt;getPixelsRef());
            }
        }</code></pre>
<p>Mode draw:</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp">        <span class="kw">case</span> SPIKYBLOBSLITSCAN:
        {
            <span class="co">//do some SPIKYBLOBSLITSCAN - used the slit scan to hold it....</span>
            ofSetColor(<span class="dv">255</span>,<span class="dv">255</span>,<span class="dv">255</span>);
            slitScan.getOutputImage().draw(<span class="dv">0</span>, <span class="dv">0</span>, ofGetWidth(), ofGetHeight());
            <span class="co">//theSpikey.theFBO.draw(0,0,ofGetWidth(), ofGetHeight());</span>
        }
            <span class="kw">break</span>;</code></pre>
<h4 id="mirrorkaleidoscope">MIRRORKALEIDOSCOPE</h4>
<div class="figure">
<div style="clear:both"><a href="..\images\project_joel/images/grabs/17_MIRRORKALEIDOSCOPE.jpg" target="_blank"><img alt="MIRRORKALEIDOSCOPE Mode" src="..\images\project_joel/images/grabs/17_MIRRORKALEIDOSCOPE.jpg" title="MIRRORKALEIDOSCOPE Mode"/></a></div><span class="caption">MIRRORKALEIDOSCOPE Mode</span>
</div>
<p>Combining Mirror and Kaleidoscope modes.</p>
<p>Mode update:</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp">        <span class="kw">case</span> MIRRORKALEIDOSCOPE: <span class="co">//MIRRORKALEIDOSCOPE mirror</span>
        {
            <span class="kw">if</span>(timeline.getVideoPlayer(<span class="st">"video"</span>)-&gt;isFrameNew()){
                verticalMirrorImage.setFromPixels(timeline.getVideoPlayer(<span class="st">"video"</span>)-&gt;getPixels(), verticalMirrorImage.getWidth(), verticalMirrorImage.getHeight());
                
                verticalMirrorImage.updateTexture();
                
                kaleidoscopeMirrorImage.setFromPixels(timeline.getVideoPlayer(<span class="st">"video"</span>)-&gt;getPixels(), kaleidoscopeMirrorImage.getWidth(), kaleidoscopeMirrorImage.getHeight());
                
                kaleidoscopeMirrorImage.updateTexture();
            }
        }
            <span class="kw">break</span>;</code></pre>
<p>Mode draw:</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp">        <span class="kw">case</span> MIRRORKALEIDOSCOPE:
        {
            <span class="dt">bool</span> usingNormTexCoords = ofGetUsingNormalizedTexCoords();
            
            <span class="kw">if</span>(!usingNormTexCoords) {
                ofEnableNormalizedTexCoords();
            }
            
            verticalMirrorImage.getTextureReference().bind();
            
            ofMesh mirrorMesh;
            mirrorMesh.clear();
            mirrorMesh.addVertex(ofVec3f(<span class="dv">0</span>, <span class="dv">0</span>));
            mirrorMesh.addVertex(ofVec3f(<span class="dv">0</span>, ofGetHeight()));
            mirrorMesh.addVertex(ofVec3f(ofGetWidth()/<span class="dv">2</span>, <span class="dv">0</span>));
            mirrorMesh.addVertex(ofVec3f(ofGetWidth()/<span class="dv">2</span>, ofGetHeight()));
            mirrorMesh.addVertex(ofVec3f(ofGetWidth(), <span class="dv">0</span>));
            mirrorMesh.addVertex(ofVec3f(ofGetWidth(), ofGetHeight()));
            
            
            mirrorMesh.addTexCoord(ofVec2f(<span class="fl">0.</span><span class="dv">25</span>, <span class="fl">0.</span><span class="dv">0</span>));
            mirrorMesh.addTexCoord(ofVec2f(<span class="fl">0.</span><span class="dv">25</span>, <span class="fl">1.</span><span class="dv">0</span>));
            mirrorMesh.addTexCoord(ofVec2f(<span class="fl">0.</span><span class="dv">75</span>, <span class="fl">0.</span><span class="dv">0</span>));
            mirrorMesh.addTexCoord(ofVec2f(<span class="fl">0.</span><span class="dv">75</span>, <span class="fl">1.</span><span class="dv">0</span>));
            mirrorMesh.addTexCoord(ofVec2f(<span class="fl">0.</span><span class="dv">25</span>, <span class="fl">0.</span><span class="dv">0</span>));
            mirrorMesh.addTexCoord(ofVec2f(<span class="fl">0.</span><span class="dv">25</span>, <span class="fl">1.</span><span class="dv">0</span>));
            
            mirrorMesh.setMode(OF_PRIMITIVE_TRIANGLE_STRIP);
            ofSetColor(ofColor::white);
            mirrorMesh.draw();
            
            verticalMirrorImage.getTextureReference().unbind();
            
            kaleidoscopeMirrorImage.getTextureReference().bind();
            
            <span class="dt">int</span> star = ((<span class="dt">int</span>)timeline.getValue(<span class="st">"star"</span>)*<span class="dv">2</span>);<span class="co">//8; //get star from the timeline gui, but multiply by 2 to get to always even</span>
            <span class="dt">float</span> offset = timeline.getValue(<span class="st">"offset"</span>);<span class="co">//0.5f; // get offset from the timeline gui</span>
            <span class="dt">float</span> angle = <span class="fl">360.f</span>/star; <span class="co">//8 sides to start</span>
            
			ofMesh mesh;
            
			ofVec3f vec(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>);
			mesh.addVertex(vec);
			vec.x += ofGetHeight()/<span class="dv">2</span>;
            
			<span class="kw">for</span>(<span class="dt">int</span> i = <span class="dv">0</span>; i &lt; star; i++) {
				mesh.addVertex(vec);
				vec.rotate(angle, ofVec3f(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>));
			}
            
			<span class="co">// close the loop</span>
			mesh.addVertex(vec);

			<span class="co">// now work out the texcoords</span>
			<span class="co">/*</span>
<span class="co">			 __________________</span>
<span class="co">			 |   \        /   |</span>
<span class="co">			 |    \      /    |</span>
<span class="co">			 |     \    /     |</span>
<span class="co">			 |      \  /      |</span>
<span class="co">			 |       \/       |</span>
<span class="co">			 +----------------+</span>

<span class="co">			 A v shape out of the centre of the camera texture</span>
<span class="co">			 */</span>

			<span class="dt">float</span> realOffset = <span class="fl">0.</span><span class="dv">5</span>;
			<span class="co">// normalized distance from the centre (half the width of the above 'V')</span>
			<span class="dt">float</span> dist = ABS((<span class="dt">float</span>)kaleidoscopeMirrorImage.getHeight()*tan(ofDegToRad(angle)*<span class="fl">0.</span><span class="dv">5</span>))/(<span class="dt">float</span>)kaleidoscopeMirrorImage.getHeight();

			<span class="co">// the realOffset is where the (normalized) middle of the 'V' is on the x-axis</span>
			realOffset = ofMap(offset, <span class="dv">0</span>, <span class="dv">1</span>, dist, <span class="dv">1</span>-dist);

			<span class="co">// this is the point at the bottom of the triangle - our centre for the triangle fan</span>
			mesh.addTexCoord(ofVec2f(realOffset, <span class="dv">1</span>));

			ofVec2f ta(realOffset-dist, <span class="dv">0</span>);
			ofVec2f tb(realOffset+dist, <span class="dv">0</span>);
			<span class="kw">for</span>(<span class="dt">int</span> i = <span class="dv">0</span>; i &lt;= star; i++) {
				<span class="kw">if</span>(i%<span class="dv">2</span>==<span class="dv">0</span>) {
					mesh.addTexCoord(ta);
				} <span class="kw">else</span> {
					mesh.addTexCoord(tb);
				}
			}

			glPushMatrix();
			glTranslatef(ofGetWidth()/<span class="dv">2</span>, ofGetHeight()/<span class="dv">2</span>, <span class="dv">0</span>);
			mesh.setMode(OF_PRIMITIVE_TRIANGLE_FAN);
			mesh.draw();
			glPopMatrix();

            kaleidoscopeMirrorImage.getTextureReference().unbind();

            <span class="co">// pop normalized tex coords</span>
            <span class="kw">if</span>(!usingNormTexCoords) {
                ofDisableNormalizedTexCoords();
            }

            <span class="co">//white fur</span>
            ofEnableAlphaBlending();
            flowSolver.drawGrey(ofGetWidth(),ofGetHeight(), <span class="dv">10</span>, <span class="dv">3</span>);
            ofDisableAlphaBlending();
        }
            <span class="kw">break</span>;</code></pre>
<h4 id="particles">PARTICLES</h4>
<div class="figure">
<div style="clear:both"><a href="..\images\project_joel/images/grabs/18_PARTICLES.jpg" target="_blank"><img alt="PARTICLES Mode" src="..\images\project_joel/images/grabs/18_PARTICLES.jpg" title="PARTICLES Mode"/></a></div><span class="caption">PARTICLES Mode</span>
</div>
<p>Using Somantics particle functionality as a SlitScan delay map.</p>
<p>Mode update:</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp">        <span class="kw">case</span> PARTICLES:
        {
            <span class="co">//PARTICLES</span>
            theParticles.update(&amp;depthContours);
            theParticles.draw(ofColor::white);
            ofImage distortionMap;
            distortionMap.allocate(theParticles.theFBO.getWidth(), theParticles.theFBO.getHeight(), OF_IMAGE_COLOR);
            theParticles.theFBO.readToPixels(distortionMap.getPixelsRef());
            distortionMap.resize(timeline.getVideoPlayer(<span class="st">"video"</span>)-&gt;getWidth(), timeline.getVideoPlayer(<span class="st">"video"</span>)-&gt;getHeight());
            slitScan.setDelayMap(distortionMap);
            <span class="kw">if</span>(timeline.getVideoPlayer(<span class="st">"video"</span>)-&gt;isFrameNew()){
                slitScan.addImage(timeline.getVideoPlayer(<span class="st">"video"</span>)-&gt;getPixelsRef());
            }
        }
            <span class="kw">break</span>;</code></pre>
<p>Mode draw:</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp">        <span class="kw">case</span> PARTICLES:
        {
            <span class="co">//do some PARTICLES - used the slit scan to hold it....</span>
            ofSetColor(<span class="dv">255</span>,<span class="dv">255</span>,<span class="dv">255</span>);
            slitScan.getOutputImage().draw(<span class="dv">0</span>, <span class="dv">0</span>, ofGetWidth(), ofGetHeight());
            <span class="co">//theParticles.theFBO.draw(0,0,ofGetWidth(), ofGetHeight());</span>
        }
            <span class="kw">break</span>;</code></pre>
<h4 id="whitefur">WHITEFUR</h4>
<div class="figure">
<div style="clear:both"><a href="..\images\project_joel/images/grabs/19_WHITEFUR.jpg" target="_blank"><img alt="WHITEFUR Mode" src="..\images\project_joel/images/grabs/19_WHITEFUR.jpg" title="WHITEFUR Mode"/></a></div><span class="caption">WHITEFUR Mode</span>
</div>
<p>Turning the ofxOpticalFlowFarneback demo code, but making the graphical output monochrome.</p>
<p>Mode update:</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp">        <span class="kw">case</span> WHITEFUR: <span class="co">//WHITEFUR, nowt</span>
        {
        }
            <span class="kw">break</span>;</code></pre>
<p>Mode draw:</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp">        <span class="kw">case</span> WHITEFUR:
        {
            ofSetColor(ofColor::white);
            timeline.getVideoPlayer(<span class="st">"video"</span>)-&gt;draw(<span class="dv">0</span>, <span class="dv">0</span>, ofGetWidth(),ofGetHeight());
            ofEnableAlphaBlending();
            flowSolver.drawGrey(ofGetWidth(),ofGetHeight(), <span class="dv">10</span>, <span class="dv">3</span>);
            ofDisableAlphaBlending();
        }
            <span class="kw">break</span>;</code></pre>
<h4 id="paint">PAINT</h4>
<div class="figure">
<div style="clear:both"><a href="..\images\project_joel/images/grabs/11_PAINT.jpg" target="_blank"><img alt="PAINT Mode" src="..\images\project_joel/images/grabs/11_PAINT.jpg" title="PAINT Mode"/></a></div><span class="caption">PAINT Mode</span>
</div>
<p>Porting the Paint mode from Somantics as a delay map.</p>
<p>Mode update:</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp">        <span class="kw">case</span> PAINT: <span class="co">//body painting diff</span>
        {
            slitScan.setDelayMap(paintCanvasAsOfImage);
            <span class="kw">if</span>(timeline.getVideoPlayer(<span class="st">"video"</span>)-&gt;isFrameNew()){
                slitScan.addImage(timeline.getVideoPlayer(<span class="st">"video"</span>)-&gt;getPixelsRef());
            }
        }
            <span class="kw">break</span>;</code></pre>
<p>Mode draw:</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp">        <span class="kw">case</span> PAINT:
        {
            <span class="co">//do some paint - used the slit scan to hold it....</span>
            slitScan.getOutputImage().draw(<span class="dv">0</span>, <span class="dv">0</span>, ofGetWidth(), ofGetHeight());
        }
            <span class="kw">break</span>;</code></pre>
<h2 id="appendix-3-edited-development-notes">Appendix 3: Edited development notes</h2>
<h4 id="th-may-2013">29th May 2013</h4>
<p>oF/of_v0.7.4_osx_release/apps/ofxKinect-demos</p>
<p>Also downloaded ofxKinect. Get gui working first, with ofxKinect, then start on:</p>
<ul>
<li>https://github.com/toruurakawa/ofxFakeMotionBlur</li>
<li>Don't use, use jamezilla https://github.com/kylemcdonald/ofxBlur</li>
<li>https://github.com/jamezilla/ofxBlurShader</li>
<li>https://github.com/kylemcdonald/ofxCameraFilter</li>
<li>https://github.com/vanderlin/ofxBox2d</li>
<li>https://github.com/NickHardeman/ofxBullet</li>
<li>https://github.com/fishkingsin/ofxPBOVideoPlayer</li>
<li>https://github.com/arturoc/ofxPlaymodes</li>
<li>Don't use, in core now https://github.com/Flightphase/ofxQTKitVideoPlayer</li>
<li>https://github.com/after12am/ofxTLGlitch</li>
<li>https://github.com/bakercp/ofxVideoBuffer</li>
<li>https://github.com/bakercp/ofxVideoUtils</li>
<li>https://github.com/obviousjim/ofxSlitScan</li>
</ul>
<p>Doing gui - having to make the projectGenerator to make the projects, generating the examples now.... Recopy over examples after! Did it, just copying in the empty example xcode project, all in here now:</p>
<p>oF/of_v0.7.4_osx_release/examples/gui</p>
<p>Email of notes on development:</p>
<p>On 29 May 2013, at 20:44, Joel Gethin Lewis wrote:</p>
<ul>
<li>All ofFloatColor or ofFloatImages</li>
<li>HSB blob shifts as a mode - crazy colours, also try whole image on slow change using ofmath demos</li>
<li>Blobs cracking off</li>
<li>Just blackness on blob</li>
<li>Slit scan obvs</li>
<li>ofxbox2d? Kinect demos? Look at memos</li>
<li>Look at ofxaddons for time ones</li>
<li>Use ofGui official one</li>
<li>Have different GUI panes per constructor for ofxScenes (make that)</li>
</ul>
<p>Think in addon way - indeed that every scene might have addons inside it. That's the way the should be. Addons inside scenes. Scenes are subclasses of ofxScene. Draw it out. Start with slitscan as first scene. Just get that working then use that as basis for ofxScene. Pragmatic! Will need central image creator as input for each scene. Kinect in this case. Don't worry about that for now.</p>
<p>ofparameter is missing! Looking at old OF folder from other project: openFrameworks-develop/apps/devApps/projectGenerator, looking in there in the oF project to try to find what is going on...</p>
<p>openFrameworks-develop/libs/openFrameworks/types contains:</p>
<ul>
<li>ofBaseTypes.cpp</li>
<li>ofBaseTypes.h</li>
<li>ofColor.cpp</li>
<li>ofColor.h</li>
<li>ofParameter.cpp</li>
<li>ofParameter.h</li>
<li>ofParameterGroup.cpp</li>
<li>ofParameterGroup.h</li>
<li>ofPoint.cpp</li>
<li>ofPoint.h</li>
<li>ofRectangle.cpp</li>
<li>ofRectangle.h</li>
<li>ofTypes.h</li>
</ul>
<p>looking for ofpanel</p>
<p>openFrameworks-develop/addons/ofxGui/src</p>
<p>is where it is...opening:</p>
<p>oF/of_v0.7.4_osx_release/examples/gui/guiExample</p>
<p>again, just trying to add it in, in the addon... Nooo thats bad.. should use the develop version... space is low...now working here:</p>
<p>oF/openFrameworks-develop/apps/devApps/projectGenerator</p>
<p>trying to build that and run it, had to select the root oF folder, it was defaulting to a weird one, so selected:</p>
<p>oF/openFrameworks-develop</p>
<p>seems to be working, leaving it for a bit...</p>
<p>error ofFile::copyFromTo source file/folder doesn't exist: oF/openFrameworks-develop/scripts/osx/template/emptyExample.xcodeproj/xcshareddata/WorkspaceSettings.xcsettings</p>
<p>is the error....it's correct:</p>
<p>oF/openFrameworks-develop/scripts/osx/template/emptyExample.xcodeproj/xcshareddata</p>
<p>doesn't have it</p>
<p>openFrameworks-develop/apps/devApps/projectGenerator/bin/data/xcode/template/emptyExample.xcodeproj/xcshareddata</p>
<p>copied that in, and another file inside</p>
<p>openFrameworks-develop/apps/devApps/projectGenerator/bin/data/xcode/template/emptyExample.xcodeproj/xcshareddata/xcschemes</p>
<p>xcschememanagement.plist</p>
<p>as well...so trying to generate again...seems to be working now......won't paste in the log! (-; trying this now...</p>
<p>oF/openFrameworks-develop/examples/gui/guiExample</p>
<p>nice!</p>
<p>oF/openFrameworks-develop/examples/gui/guiFromParametersExample</p>
<p>next - not that interesting...</p>
<p>oscParametersReceiver oscParametersSender</p>
<p>together... Neat demo! synchronised gui controls....both crash on exit</p>
<p>sender:</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp">
<span class="dt">void</span> testApp::setup(){
	parameters.setName(<span class="st">"parameters"</span>);
	parameters.add(size.set(<span class="st">"size"</span>,<span class="dv">10</span>,<span class="dv">1</span>,<span class="dv">100</span>));
	parameters.add(number.set(<span class="st">"number"</span>,<span class="dv">10</span>,<span class="dv">1</span>,<span class="dv">100</span>));
	parameters.add(check.set(<span class="st">"check"</span>,<span class="kw">false</span>));
	parameters.add(color.set(<span class="st">"color"</span>,ofColor(<span class="dv">127</span>),ofColor(<span class="dv">0</span>,<span class="dv">0</span>),ofColor(<span class="dv">255</span>)));
	gui.setup(parameters);
	<span class="co">// by now needs to pass the gui parameter groups since the panel internally creates it's own group</span>
	sync.setup((ofParameterGroup&amp;)gui.getParameter(),<span class="dv">6667</span>,<span class="st">"localhost"</span>,<span class="dv">6666</span>);
	ofSetVerticalSync(<span class="kw">true</span>);
}

<span class="dt">void</span> testApp::update(){
	sync.update();
}</code></pre>
<p>receiver:</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp">
<span class="dt">void</span> testApp::setup(){
	parameters.setName(<span class="st">"parameters"</span>);
	parameters.add(size.set(<span class="st">"size"</span>,<span class="dv">10</span>,<span class="dv">0</span>,<span class="dv">100</span>));
	parameters.add(number.set(<span class="st">"number"</span>,<span class="dv">10</span>,<span class="dv">0</span>,<span class="dv">100</span>));
	parameters.add(check.set(<span class="st">"check"</span>,<span class="kw">false</span>));
	parameters.add(color.set(<span class="st">"color"</span>,ofColor(<span class="dv">127</span>),ofColor(<span class="dv">0</span>,<span class="dv">0</span>),ofColor(<span class="dv">255</span>)));
	gui.setup(parameters);
	<span class="co">// by now needs to pass the gui parameter groups since the panel internally creates it's own group</span>
	sync.setup((ofParameterGroup&amp;)gui.getParameter(),<span class="dv">6666</span>,<span class="st">"localhost"</span>,<span class="dv">6667</span>);
	ofSetVerticalSync(<span class="kw">true</span>);
}

<span class="dt">void</span> testApp::update(){
	sync.update();
}

<span class="dt">void</span> testApp::draw(){
	gui.draw();
	ofSetColor(color);
	<span class="kw">for</span>(<span class="dt">int</span> i=<span class="dv">0</span>;i&lt;number;i++){
		ofCircle(ofGetWidth()*<span class="fl">.5</span>-size*((number<span class="dv">-1</span>)*<span class="fl">0.</span><span class="dv">5</span>-i), ofGetHeight()*<span class="fl">.5</span>, size);
	}
}</code></pre>
<p>subtle difference in port lines in sync setups...</p>
<p>oF/openFrameworks-develop/examples/gui/parameterEdgeCasesExample</p>
<p>doesn't work...</p>
<p>oF/openFrameworks-develop/examples/gui/parameterGroupExample</p>
<p>Is very intersting - two renderers running at once! Only thing missing is multiple parameters, and images being drawn? could always do that with bools, and the images being displayed on top, sliders and the like could work with that too... Moving big greenpeace video into:</p>
<p>oF/openFrameworks-develop/examples/video/osxHighPerformanceVideoPlayerExample/bin/data/movies</p>
<p>to save space, rather than copying!</p>
<p>oF/openFrameworks-develop/examples/video/osxHighPerformanceVideoPlayerExample</p>
<p>Trying this now...builds with standard movie file in demo, fingers.mov. Now trying, Greenpeace.m4v - works great! audio back too...and pixel access! MOVED video file out of the folder for safety..</p>
<p>copied in this:</p>
<p>oF/openFrameworks-develop/apps/ofxKinect-demos</p>
<p>trying normal ofxKinect first...</p>
<p>oF/openFrameworks-develop/addons/ofxKinect oF/openFrameworks-develop/addons/ofxKinect/example</p>
<p>trying that... works fine, with motor and everything...so making a mega mix up of:</p>
<p>ofxKinect, ofxGUI and ofHighPerformanceVideo demo</p>
<p>oF/openFrameworks-develop/apps/HAndLGreenpeace/001fromofxKinectExampleAndofxGUI/bin/data/movies</p>
<p>copied that in, changed name to:</p>
<p>oF/openFrameworks-develop/apps/HAndLGreenpeace/001fromofxKinectExampleAndofxGUIAndHighPerformanceVideo</p>
<p>oF/openFrameworks-develop/examples/video/osxHighPerformanceVideoPlayerExample oF/openFrameworks-develop/examples/gui/guiExample</p>
<p>copying over gui data...that works with gui.. now lets try with high performance video...all works! nice debug screen! saved it out to making of....</p>
<div class="figure">
<div style="clear:both"><a href="..\images\project_joel/images/2013_05_29_MashingHighPerfAndKinectAndGUI.jpg" target="_blank"><img alt="Kinect, GUI and High Performance Video Debug Screen" src="..\images\project_joel/images/2013_05_29_MashingHighPerfAndKinectAndGUI.jpg" title="Kinect, GUI and High Performance Video Debug Screen"/></a></div><span class="caption">Kinect, GUI and High Performance Video Debug Screen</span>
</div>
<h4 id="th-may-2013-1">30th May 2013</h4>
<p>Doing modes, tidying up gui, need to do more on gui tidy up and keys. Made:</p>
<p>oF/openFrameworks-develop/apps/HAndLGreenpeace/002FirstModesAndSlitScan</p>
<p>Builds (-; Pixel drawing is messed up, checking the original high perf demo. Recoded nicely with bits that made sense and were easier to understand...now slitsan! fixed a few gui bugs...</p>
<p>oF/openFrameworks-develop/addons/ofxSlitScan</p>
<p>Made that... Image is PNG RGB for slitscan delay map, so kinect depth map is good for that... right? (-; lucky birthday boy! works great...did grab..</p>
<div class="figure">
<div style="clear:both"><a href="..\images\project_joel/images/2013_05_31_BirthdayPresentSlitScanFromDepthImage.jpg" target="_blank"><img alt="Slit scan generated from Kinect Depth Map Slice" src="..\images\project_joel/images/2013_05_31_BirthdayPresentSlitScanFromDepthImage.jpg" title="Slit scan generated from Kinect Depth Map Slice"/></a></div><span class="caption">Slit scan generated from Kinect Depth Map Slice</span>
</div>
<h4 id="st-may-2013">31st May 2013</h4>
<p>Showed Pete, performance better on his laptop, no optimisation yet, tried out some maps with bafic...</p>
<h4 id="th-june-2013">6th June 2013</h4>
<p>First lets do GUI that corresponds to big maps, so we can switch between them... Duplicating multicoloured magic into the folder, so got all that lovely code to work with.. After lunch lets look at mirror Somantics code... Too complicated for now, need some time to sit down and make it work... For now on the Virgin SA flight, lets try some of the addons!</p>
<p>ofxBlur By Kyle McDonald had to add accelerate, qtkit and corevideo frameworks to make it work..</p>
<p>ofxBlurShader This is a very lightly refactored version of Kyle McDonald's ofBlur example (https://github.com/kylemcdonald/SubdivisionOfRoam/tree/master/GaussianBlur). It hasbeen updated to OF 007. Didn't build!</p>
<p>ofxBox2d trying every example:</p>
<ul>
<li>oF/openFrameworks-develop/addons/ofxBox2d/ComplexPolgonExample - useful for making shatter like effects - how do I texture them?</li>
<li>oF/openFrameworks-develop/addons/ofxBox2d/ContactListenerExample - useful for tiggering audio samples on ofxBox2D interactions</li>
<li>oF/openFrameworks-develop/addons/ofxBox2d/CustomDataExample - useful for storing data withing objects, nothing particularly useful there for us at the moment..</li>
<li>oF/openFrameworks-develop/addons/ofxBox2d/ForcesExample - bunch of forces on mouse action</li>
<li>oF/openFrameworks-develop/addons/ofxBox2d/JointExample -long chain of pieces</li>
<li>oF/openFrameworks-develop/addons/ofxBox2d/ofxBox2dExample - line that you can draw and lots of various shapes</li>
<li>oF/openFrameworks-develop/addons/ofxBox2d/PolygonExample - more line drawing...</li>
<li>oF/openFrameworks-develop/addons/ofxBox2d/SimpleExample - simple!</li>
</ul>
<p>ofxBullet</p>
<ul>
<li>oF/openFrameworks-develop/addons/ofxBullet/SimpleExample - is simple, several different basic shapes...</li>
<li>oF/openFrameworks-develop/addons/ofxBullet/CustomShapesExample - needs ofxAssimpMeshHelper - that is in the assimp model loader addon... cool - very fast simulation and * loading of custom shapes - perhaps pete could make custom 3D shapes?</li>
<li>oF/openFrameworks-develop/addons/ofxBullet/EventsExample - smashing of objects into each other, mouse animation of objects within cube</li>
<li>oF/openFrameworks-develop/addons/ofxBullet/JointsExample - has long chain of shapes, similar to ofxbox2d example...</li>
</ul>
<p>ofxCameraFilter</p>
<ul>
<li>oF/openFrameworks-develop/addons/ofxCameraFilter/example-graphics - simple camera effects on some rotating shapes, aberation and the like...</li>
<li>oF/openFrameworks-develop/addons/ofxCameraFilter/example-live - does the same but live, and with an interesting use of an ofMesh</li>
</ul>
<p>ofxFakeMotionBlur</p>
<ul>
<li>oF/openFrameworks-develop/addons/ofxFakeMotionBlur/example - no work</li>
</ul>
<p>ofxPBOVideoPlayer</p>
<ul>
<li>oF/openFrameworks-develop/addons/ofxPBOVideoPlayer/example - seems speedy</li>
</ul>
<p>ofxPlaymodes</p>
<ul>
<li>oF/openFrameworks-develop/addons/ofxPlaymodes/example-pmAV - needs more addons, come back to this..</li>
</ul>
<p>ofxTLGlitch</p>
<ul>
<li>trying oF/openFrameworks-develop/addons/ofxVideoBuffer/example-multi-tap - had to add empty example, couldn't get building....</li>
</ul>
<p>DONE addon off....</p>
<h4 id="th-june-2013-1">12th June 2013</h4>
<p>Greenpeace logos look nice as slit scans! saved all graphics and fonts into:</p>
<p>2013_06_12_Font 2013_06_12_GreenpeaceLogos</p>
<p>Pete gave me new audio and the film for working with duration</p>
<h4 id="th-june-2013-2">13th June 2013</h4>
<p>Duration demo is up from James George too:</p>
<ul>
<li>Posted demo code from Duration.cc demo github.com/obviousjim/Dur… cc <span class="citation">@JGL</span></li>
</ul>
<p>got that, put it here:</p>
<p>2013_06_13_obviousJimAudioReactiveRing</p>
<p>and copied into:</p>
<p>OF/openFrameworks-develop/apps/jamesGeorgeDurationDemo/DurationAudioReactiveRing-master</p>
<p>made:</p>
<p>Duration_004_OSX Duration_004_OSX.zip durationData</p>
<p>too.. The readme sez:</p>
<pre><code>
Duration: Timeline for Creative Code Demonstration

Code used in the demo of Duration:
http://vimeo.com/59654979

Requires ofxRange and ofxDuration
https://github.com/YCAMInterlab/ofxDuration
https://github.com/Flightphase/ofxRange

Download Duration
http://www.duration.cc/ // https://github.com/YCAMInterlab/Duration

Supported by YCAM InterLab Guest Research Project 2012
</code></pre>
<p>Getting those.. put in here:</p>
<p>2013_06_13_MoreDurationBits</p>
<p>ofxRange-master.zip ofxDuration-master.zip</p>
<p>trying this first</p>
<p>OF/openFrameworks-develop/addons/ofxDuration/example-simpleReceiver</p>
<p>totally did it, totally worked - have to show Pete Hellicar it tomorrow, and disuss which controls he wants...made a new track:</p>
<p>Duration/durationData/FirstTry</p>
<p>audio all loaded in fine (-; need to test with film sync, see if that works OK.. try to set the movie time on each frame? will it fuck everything? Basically should make a new version of the app:</p>
<p>OF/openFrameworks-develop/apps/HAndLGreenpeace/003WithOFXDuration</p>
<p>Added:</p>
<p>GUI SimpleReceiverPort.txt</p>
<p>to data folder too...need to compare with: OF/openFrameworks-develop/addons/ofxDuration/example-simpleReceiver and duplicate the functionality - start with scene control and colour....</p>
<p>MORNING TIME</p>
<p>OF/openFrameworks-develop/addons/ofxDuration/example-simpleReceiver</p>
<p>opening that and taking the functionality over...</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp">
ofxDurationTrack sceneTrack = duration.getTrack(<span class="st">"/scene"</span>);
string currentScene = sceneTrack.flag;

<span class="kw">if</span>(currentScene == <span class="st">"VIDEO"</span>){
    currentMode = VIDEO;
}

<span class="kw">if</span>(currentScene == <span class="st">"SLITSCANBASIC"</span>){
    currentMode = SLITSCANBASIC;
}</code></pre>
<p>totally works!</p>
<h4 id="th-june-2013-3">16th June 2013</h4>
<p>Lets try the video syncing over osc.. Didn't seem to work with:</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp"><span class="dt">float</span> remoteTime = sceneTrack.lastUpdatedTime;
cout &lt;&lt; <span class="st">"Remote time is:"</span> &lt;&lt; remoteTime &lt;&lt; endl
<span class="dt">float</span> totalLengthOfVideo = greenpeaceVideo.getDuration();
<span class="dt">float</span> percentToSeekTo = remoteTime/totalLengthOfVideo;
greenpeaceVideo.setPosition(percentToSeekTo);</code></pre>
<p>Hmmm. Sent this to james and got a response:</p>
<p>On 16 Jun 2013, at 19:00, James George wrote: yea it's impossible to call setPosition on a video every frame and have it playback smoothly. Quicktime needs to control its own time. Try this: play the video back normally in openframeworks and then update Duration every frame based on it's position:</p>
<pre><code>https://github.com/YCAMInterlab/Duration#controlling-duration-through-osc

Specifically make sure Duration has its incoming OSC port set and from OF send it a /duration/seektosecond. Get the seconds from the video player.getPosition()*player.getDuration() then create an outgoing OSC message directed at Duration:

Seek	/duration/seektosecond	Second (Float)	 Sets playhead position to the specified second
Sending the /seektosecond message will then trigger an update to come back from Duration to your app and update all the other params.</code></pre>
<p>On Sun, Jun 16, 2013 at 1:49 PM, Joel Gethin Lewis wrote: Hey James, I've been trying to get a Duration app to be able to sync the video playback on an OF app - I used your example and have started trying to sync to the time from a track:</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp">ofxDurationTrack sceneTrack = duration.getTrack(<span class="st">"/scene"</span>);
string currentScene = sceneTrack.flag;
<span class="dt">float</span> remoteTime = sceneTrack.lastUpdatedTime;
cout &lt;&lt; <span class="st">"Remote time is:"</span> &lt;&lt; remoteTime &lt;&lt; endl;
<span class="dt">float</span> totalLengthOfVideo = greenpeaceVideo.getDuration();
cout &lt;&lt; <span class="st">"totalLengthOfVideo time is:"</span> &lt;&lt; totalLengthOfVideo &lt;&lt; endl;
<span class="dt">float</span> percentToSeekTo = remoteTime/totalLengthOfVideo;
cout &lt;&lt; <span class="st">"percentToSeekTo time is:"</span> &lt;&lt; percentToSeekTo &lt;&lt; endl;
greenpeaceVideo.setPosition(percentToSeekTo);</code></pre>
<p>But it results in stuttering, playback - do you have any tips? How often are the control packets sent? Should I be getting the remote time in a better way? Cheers, Joel</p>
<p>looking at:</p>
<p>https://github.com/YCAMInterlab/Duration#controlling-duration-through-osc</p>
<p>/duration/seektosecond</p>
<p>Is what we want...so need to setup osc, trying to get that working with a simple sender, having problems gaining control from the OF app. Sent this:</p>
<p>On 16 Jun 2013, at 20:48, Joel Gethin Lewis wrote: Hey James, Thanks! It kind of works, but not really. I have my app jumping around it's video when I press t:</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp"><span class="kw">case</span> <span class="st">'t'</span>:
{
   <span class="dt">float</span> newseekposition = (<span class="dt">float</span>)mouseX/(<span class="dt">float</span>)ofGetWidth();
   ofClamp(newseekposition, <span class="fl">0.f</span>, <span class="fl">1.f</span>); <span class="co">//safety</span>
   greenpeaceVideo.setPosition(newseekposition);
   cout &lt;&lt; <span class="st">"New seek position is: "</span> &lt;&lt; newseekposition &lt;&lt; endl;
}</code></pre>
<p>If the Duration app is set to paused, it updates fine, the playhead moving around when I press T in my app- but I don't get the messages back from Duration! If it isn't paused (the duration app), I get the messages, but I can't move the Duration playhead around with the above messages! Catch 22? What should I do? I want to get the messages back, have it be playing on both ends and be able to seek. At the moment, I can have seeking in my app and Duration, but without messages back. Or messages back, without seeking.</p>
<p>Sending the /seektosecond message will then trigger an update to come back from Duration to your app and update all the other params.</p>
<p>Doesn't seem to be happening? Two little Duration suggestions:</p>
<ol style="list-style-type: decimal">
<li>Shouldn't ofxDuration have a send to DurationApp method? That would be useful, no? Auto osc.</li>
<li>Can I mute the audio of the Duration app in its GUI?</li>
</ol>
<p>Any thoughts gratefully recieved. Ideally, I'd like either side to be Master if it sends messages to the other. Make sense? My app the true master, but seeking to Quicktime if it gets an occasional timeline change from the Duration app - but how to do that only some of the time? Cheers, Joel</p>
<p>Made new osc send:</p>
<p>On 16 Jun 2013, at 20:51, Joel Gethin Lewis wrote: This is my send, in my update:</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp"><span class="co">//update duration based on the position of the quicktime player</span>
<span class="dt">float</span> videoTimeToSend = greenpeaceVideo.getPosition()*greenpeaceVideo.getDuration();
ofxOscMessage m;
m.setAddress(<span class="st">"/duration/seektosecond"</span>);
m.addFloatArg(videoTimeToSend);
senderToDuration.sendMessage(m);</code></pre>
<p>Got this reply back, and replied:</p>
<p>On 16 Jun 2013, at 21:07, Joel Gethin Lewis wrote: Hey James, I'll take a look. BUT! Looking at this video:</p>
<p>https://vimeo.com/59653952</p>
<p>It looks like I might be better off doing everything in a single OF app. What do you think? Do you think my massive video file (785,526,769 bytes (785.5 MB on disk)) will kill your thumbnail maker? Lets see...Cheers, Joel</p>
<p>On 16 Jun 2013, at 20:53, James George wrote:</p>
<p>Huh! Duration should definitely update when you move the playhead even if it's not playing... Definitely a bug. Must be a bug in the way seektosecond works. This may be a rabbit hole, but try downloading the source from the Duration website (its the entire OF bundle) and see if you can give it a look. it's probalby a simple change to make sure that handleOscOut() works even when it's not playing.</p>
<p>Watched that video above, did this:</p>
<p>jglmacbookprocore2:addons joel$ git clone https://github.com/YCAMInterlab/ofxTimeline.git</p>
<p>Trying:</p>
<p>OF/openFrameworks-develop/addons/ofxTimeline/example-videoRenderer</p>
<p>Worked...tried:</p>
<p>OF/openFrameworks-develop/apps/tryingBigVideoIntoOfxTimeline/example-allTracks</p>
<p>Totally worked! OK - so need to get audio file and video files separately... autosaves...... apple z for undo even works!</p>
<h4 id="th-june-2013-4">17th June 2013</h4>
<p>On 17 Jun 2013, at 00:12, James George wrote: the video player posted on the oF list is really nice, but it doesn't support the getCurrentFrame() command which may cause some issues. give it a shot! On Sun, Jun 16, 2013 at 5:02 PM, Joel Gethin Lewis wrote: IT TOTALLY ROCKS! It totally works with the thumbnailer. Great work. I am going to code it up as a pure OF app, with maybe a little OSC Remote. Did you see the discussion about the new OSX high performance video player? How gnarly is the hookup to the ofVideoPlayer? I just glanced at the code and it didn't seem too bad.. AMAZING. Cheers, Joel On Sun, 16 Jun 2013, at 21:26, James George wrote: that's me in the video btw ;) On Sun, Jun 16, 2013 at 4:26 PM, James George wrote: no it'll be fine, the thumbnails generator is really light, it pulls only as it needs. you can also disable it.</p>
<p>Need to decide on the different modes - and do a mirror mode and a proper sparkles mode with direction.. First off, make a new version, with everything in it and timeline and duration stripped out...RIGHT! made this:</p>
<p>OF/openFrameworks-develop/apps/HAndLGreenpeace/004BuiltInOfxTimeLine</p>
<p>All working nice for demo, need to re-add GUI elements for showing and hiding etc... did it on mouse hide... works great with the slitscan control on too...So next, it's really time to do effects...mirror first.. add an x variable for the point....</p>
<h4 id="th-june-2013-5">18th June 2013</h4>
<p>doing sparkles first: copied over:</p>
<p>/Users/joel/Documents/Projects/HellicarAndLewis/greenpeaceArcticGlastonbury2013/OF/openFrameworks-develop/apps/HAndLGreenpeace/004BuiltInOfxTimeLine/bin/data/particles</p>
<ul>
<li>blob.png</li>
<li>glitter.png</li>
<li>sparkle.png</li>
<li>star.png</li>
</ul>
<p>as the images for the particles</p>
<p>https://github.com/HellicarAndLewis/MulticolouredMagic/blob/master/Somantics/src/somantics/Sparkles/Sparkles.cpp</p>
<p>Do this with the depth image as input to the blob tracker - or the IR image?</p>
<p>Get all three modes working first, then have a think about how to get them working as MODES - make a mode object? Look at somantics for how to have scenes. Maybe call it a mode? Construct with a pointer to the test app for easier data steal. Have a vector of things. Just making sparkles for now, made a sparkle cloud, duplicated the spartkcles logic from marekes sparkles from somantics - the one that spawns along the edges of the blobs... great way of doing it! Going to need an FBO to draw the Sparkles into, so looking at:</p>
<p>/Users/joel/Documents/Projects/HellicarAndLewis/greenpeaceArcticGlastonbury2013/OF/openFrameworks-develop/examples/gl/fboTrailsExample</p>
<p>Lets make it first, then do a InstallationMode object, based on what Sparkles actually needed. tightly coupling into testApp at the moment with a passed pointer, but whatever works for now...Compilation problems, forward declaration because of pointers to testApp...</p>
<p>http://stephanschulz.ca/downloads/singleton.zip</p>
<p>had a look, from :</p>
<p>http://forum.openframeworks.cc/index.php/topic,12466.msg54372.html#msg54372</p>
<p>the first one i found was about singletons which allows you to have global variables that can be accessed by all classes; i.e. all .cpp files. Bollocks to singletons... bad for test app, decoupled...trying to get the FBO to play nicely with the slitscan and the sparkles</p>
<p>FBO-&gt;SLITSCAN is working:</p>
<p>RGB fbo!</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp">ofImage distortionMap;
distortionMap.allocate(someSparkles.theFBO.getWidth(), someSparkles.theFBO.getHeight(), OF_IMAGE_COLOR);
someSparkles.theFBO.readToPixels(distortionMap.getPixelsRef());
distortionMap.resize(timeline.getVideoPlayer(<span class="st">"video"</span>)-&gt;getWidth(), timeline.getVideoPlayer(<span class="st">"video"</span>)-&gt;getHeight());
slitScan.setDelayMap(distortionMap);</code></pre>
<p>setup:</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp">theFBO.allocate(aWidth, aHeight, GL_RGB);</code></pre>
<p>draw:</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp">theFBO.begin();
ofSetColor(ofColor::black);
ofRect(<span class="dv">0</span>,<span class="dv">0</span>,theFBO.getWidth(), theFBO.getHeight());
ofSetColor(ofColor::white);
<span class="dt">float</span> circleX = theFBO.getWidth()/<span class="fl">2.f</span>;
<span class="dt">float</span> circleY = theFBO.getHeight()/<span class="fl">2.f</span>;
<span class="dt">float</span> circleRadius = min(circleX, circleY);
ofCircle(circleX,circleY, circleRadius);
theFBO.end();</code></pre>
<p>So the bug is currently with how the cloud of sparkles is being drawn - is the contour finder being read properly? I'm trying to draw at:581814,23197.4, at size:13.0935 is where things were trying to draw! co-ordinates must be in pixels inside the contour tracker! dumb......sorted it with:</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp"><span class="dt">void</span> Sparkles::update(ofxCvContourFinder* aContourFinder){
	<span class="dt">float</span> cloudWidth = theFBO.getWidth();
	<span class="dt">float</span> cloudHeight = theFBO.getHeight();

	<span class="dt">float</span> contourWidth = aContourFinder-&gt;getWidth();
	<span class="dt">float</span> contourHeight = aContourFinder-&gt;getHeight();

	<span class="dt">float</span> widthRatio = cloudWidth/contourWidth;
	<span class="dt">float</span> heightRatio = cloudHeight/contourHeight;

	<span class="co">// now just stick some particles on the contour and emit them randomly</span>
	<span class="kw">for</span>(<span class="dt">int</span> i = <span class="dv">0</span>; i &lt; aContourFinder-&gt;nBlobs; i++) {
	    <span class="dt">int</span> step = <span class="dv">10</span>;<span class="co">//contourFinder.blobs[i].pts.size()/10;</span>
	    <span class="kw">for</span>(<span class="dt">int</span> j = <span class="dv">0</span>; j &lt; aContourFinder-&gt;blobs[i].pts.size(); j+=step) {
	        cloud.spawn(
	                (aContourFinder-&gt;blobs[i].pts[j].x)*widthRatio,
	                (aContourFinder-&gt;blobs[i].pts[j].y)*heightRatio,
	                ofRandom(<span class="dv">-5</span>, <span class="dv">5</span>), ofRandom(<span class="dv">-5</span>, <span class="dv">5</span>));
	    }
	}
}</code></pre>
<p>OK that works</p>
<p>SECOND:</p>
<p>On 17 Jun 2013, at 19:01, Joel Gethin Lewis wrote: This is the mirror: https://github.com/HellicarAndLewis/MulticolouredMagic/blob/master/Somantics/src/somantics/Mirror/Mirror.cpp Just do a vertical scene and a horizontal scene for now - kaledscope later...DONE... just the vertical one for now...</p>
<p>THIRD:</p>
<p>Paint as a slitscan input. Lets do paint! it's fun... - it is fun! It looks nice....DONE Quick optimisation - why are both slitscans done separately? Changed it, all seems fine. BUG: when switching to slitscan basic from sparkles, you don't get any update of the slitscan image, but it works initially...dirty hack to make work - change prevslitscan to -1 if it's not slitscan basic mode...</p>
<p>OK next! sleep...</p>
<h4 id="th-june-2013-6">20th June 2013</h4>
<p>FOURTH:</p>
<p>Use the blobs from the depth image - make a bunch of triangles in box2d as greyscale image that floats up and ADD that to slitscan...dropped in box2d, all working ok: OF/openFrameworks-develop/apps/HAndLGreenpeace/006AddingOFXBox2D bit slow, need to look at optimising...</p>
<h4 id="st-june-2013">21st June 2013</h4>
<p>grabbing some addons:</p>
<ul>
<li>https://github.com/maxillacult/ofxPostGlitch</li>
<li>https://github.com/outsidecontext/ofxPSLevels</li>
<li>https://github.com/neilmendoza/ofxPostProcessing - http://www.neilmendoza.com/ofxpostprocessing/</li>
<li>https://github.com/julapy/ofxOpticalFlowLK</li>
<li>https://github.com/timscaffidi/ofxOpticalFlowFarneback</li>
<li>https://github.com/Flightphase/ofxCvOpticalFlowLK</li>
</ul>
<p>doing a quick look at them before supper.... lazy! look at in the morning....the next evening! ok addons first....</p>
<p>doing this:</p>
<p>ofxCvOpticalFlowLK - no readme, no draw, moving on - could use the draw image into an FBO easily</p>
<p>ofxOpticalFlowFarneback - OF/openFrameworks-develop/apps/bunchOfAddonsTrying/ofxOpticalFlowFarneback looks beautiful! definitely develop this one post stripped down version.... easy conversion to greyscale for the coloured one</p>
<p>ofxOpticalFlowLK - OF/openFrameworks-develop/apps/bunchOfAddonsTrying/ofxOpticalFlowLK similar look to ofxOpticalFlowFarneback but not as pretty, use the other...</p>
<p>ofxPostGlitch - OF/openFrameworks-develop/apps/bunchOfAddonsTrying/ofxPostGlitch lots of fun effects and already in an FBO! just do these effects on either the live video OR the depth image, but put into the slitscan</p>
<p>ofxPostProcessing 3D demo, with</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp">post.createPass&lt;FxaaPass&gt;()-&gt;setEnabled(<span class="kw">false</span>);
post.createPass&lt;BloomPass&gt;()-&gt;setEnabled(<span class="kw">false</span>);
post.createPass&lt;DofPass&gt;()-&gt;setEnabled(<span class="kw">false</span>);
post.createPass&lt;KaleidoscopePass&gt;()-&gt;setEnabled(<span class="kw">false</span>);
post.createPass&lt;NoiseWarpPass&gt;()-&gt;setEnabled(<span class="kw">false</span>);
post.createPass&lt;PixelatePass&gt;()-&gt;setEnabled(<span class="kw">false</span>);
post.createPass&lt;EdgePass&gt;()-&gt;setEnabled(<span class="kw">false</span>);</code></pre>
<p>nice, but all in 3D - doing a quick hack to draw the video grabber in the scene. No, couldn't get it working, need to draw it to a texture and draw in space, no thank you...</p>
<p>ofxPSLevels</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp">s += <span class="st">"</span><span class="ch">\n</span><span class="st">brightness (b/B) : "</span> + ofToString(levels.brightness);
s += <span class="st">"</span><span class="ch">\n</span><span class="st">contrast (c/C) : "</span> + ofToString(levels.contrast);
s += <span class="st">"</span><span class="ch">\n</span><span class="st">saturation (s/S) : "</span> + ofToString(levels.saturation);
s += <span class="st">"</span><span class="ch">\n</span><span class="st">gamma (g/G) : "</span> + ofToString(levels.gamma);</code></pre>
<p>nice to have this as a post effect for everything.</p>
<p>OK, lets get on with the other direction mirror AND the kaleidoscope..... should be relatively straight forward, just drop in for now. get rid of box2d?</p>
<p>taking out ofxbox2d - that;s better, but why is just video playback so slow? Having a look now... optimised the video only draw section...Still not fast, do it through the fucking still scan with the image that does nothing....taking out colour.....made a few more slitscans...</p>
<ul>
<li>ALLBLACK.png</li>
<li>ALLWHITE.png</li>
<li>NOHelvetica.png</li>
<li>Rewind.png</li>
<li>YesGillSans.png</li>
</ul>
<p>left in bangs..turned on snapping...they look ok, can work on these...working on horizontal mirror, made notes, did it not quite right...</p>
<p>This is wrong:</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp"><span class="kw">case</span> SLIGHTLY BUGGERED RERVERSED VERTICAL MIRROR:
{
    ofxCvColorImage mirrorImage;
    mirrorImage.allocate(timeline.getVideoPlayer(<span class="st">"video"</span>)-&gt;getWidth(), timeline.getVideoPlayer(<span class="st">"video"</span>)-&gt;getHeight());
    mirrorImage.setFromPixels(timeline.getVideoPlayer(<span class="st">"video"</span>)-&gt;getPixels(), mirrorImage.getWidth(), mirrorImage.getHeight());
    mirrorImage.updateTexture();
    <span class="dt">bool</span> usingNormTexCoords = ofGetUsingNormalizedTexCoords();
    
    <span class="kw">if</span>(!usingNormTexCoords) {
        ofEnableNormalizedTexCoords();
    }
    
    mirrorImage.getTextureReference().bind();
    
    ofMesh mesh;
    mesh.clear();
    mesh.addVertex(ofVec3f(<span class="dv">0</span>, <span class="dv">0</span>));
    mesh.addVertex(ofVec3f(<span class="dv">0</span>, ofGetHeight()));
    mesh.addVertex(ofVec3f(ofGetWidth()/<span class="dv">2</span>, <span class="dv">0</span>));
    mesh.addVertex(ofVec3f(ofGetWidth()/<span class="dv">2</span>, ofGetHeight()));
    mesh.addVertex(ofVec3f(ofGetWidth(), <span class="dv">0</span>));
    mesh.addVertex(ofVec3f(ofGetWidth(), ofGetHeight()));
    mesh.addTexCoord(ofVec2f(<span class="dv">0</span>, <span class="fl">0.</span><span class="dv">25</span>));
    mesh.addTexCoord(ofVec2f(<span class="dv">0</span>, <span class="fl">0.</span><span class="dv">75</span>));
    mesh.addTexCoord(ofVec2f(<span class="fl">1.</span><span class="dv">0</span>, <span class="fl">0.</span><span class="dv">25</span>));
    mesh.addTexCoord(ofVec2f(<span class="fl">1.</span><span class="dv">0</span>, <span class="fl">0.</span><span class="dv">75</span>));
    mesh.addTexCoord(ofVec2f(<span class="dv">0</span>, <span class="fl">0.</span><span class="dv">25</span>));
    mesh.addTexCoord(ofVec2f(<span class="dv">0</span>, <span class="fl">0.</span><span class="dv">75</span>));
    mesh.setMode(OF_PRIMITIVE_TRIANGLE_STRIP);
    ofSetColor(ofColor::white);
    mesh.draw();
    
    mirrorImage.getTextureReference().unbind();

    <span class="co">// pop normalized tex coords</span>
    <span class="kw">if</span>(!usingNormTexCoords) {
        ofDisableNormalizedTexCoords();
    }
    <span class="kw">break</span>;
}</code></pre>
<p>This is right</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp"><span class="kw">case</span> HORIZONTALMIRROR:
{
    ofxCvColorImage mirrorImage;
    
    mirrorImage.allocate(timeline.getVideoPlayer(<span class="st">"video"</span>)-&gt;getWidth(), timeline.getVideoPlayer(<span class="st">"video"</span>)-&gt;getHeight());
    mirrorImage.setFromPixels(timeline.getVideoPlayer(<span class="st">"video"</span>)-&gt;getPixels(), mirrorImage.getWidth(), mirrorImage.getHeight());
    mirrorImage.updateTexture();
    
    <span class="dt">bool</span> usingNormTexCoords = ofGetUsingNormalizedTexCoords();
    <span class="kw">if</span>(!usingNormTexCoords) {
        ofEnableNormalizedTexCoords();
    }
    
    mirrorImage.getTextureReference().bind();
    
    ofMesh mesh;
    mesh.clear();
    mesh.addVertex(ofVec3f(ofGetWidth(), <span class="dv">0</span>));
    mesh.addVertex(ofVec3f(<span class="dv">0</span>, <span class="dv">0</span>));
    mesh.addVertex(ofVec3f(ofGetWidth(), ofGetHeight()/<span class="dv">2</span>));
    mesh.addVertex(ofVec3f(<span class="dv">0</span>, ofGetHeight()/<span class="dv">2</span>));
    mesh.addVertex(ofVec3f(ofGetWidth(), ofGetHeight()));
    mesh.addVertex(ofVec3f(<span class="dv">0</span>,ofGetHeight()));
    mesh.addTexCoord(ofVec2f(<span class="fl">1.</span><span class="dv">0</span>, <span class="fl">0.</span><span class="dv">25</span>));
    mesh.addTexCoord(ofVec2f(<span class="fl">0.</span><span class="dv">0</span>, <span class="fl">0.</span><span class="dv">25</span>));
    mesh.addTexCoord(ofVec2f(<span class="fl">1.</span><span class="dv">0</span>, <span class="fl">0.</span><span class="dv">75</span>));
    mesh.addTexCoord(ofVec2f(<span class="fl">0.</span><span class="dv">0</span>, <span class="fl">0.</span><span class="dv">75</span>));
    mesh.addTexCoord(ofVec2f(<span class="fl">1.</span><span class="dv">0</span>, <span class="fl">0.</span><span class="dv">25</span>));
    mesh.addTexCoord(ofVec2f(<span class="fl">0.</span><span class="dv">0</span>, <span class="fl">0.</span><span class="dv">25</span>));
    mesh.setMode(OF_PRIMITIVE_TRIANGLE_STRIP);
    ofSetColor(ofColor::white);
    mesh.draw();
    
    mirrorImage.getTextureReference().unbind();
    
    <span class="co">// pop normalized tex coords</span>
    <span class="kw">if</span>(!usingNormTexCoords) {
        ofDisableNormalizedTexCoords();
    }
}</code></pre>
<p>Getting a bit better...fixing the controls - some of the keys were clashing. Red lines on the screen indicate track in a and out below the main timeline.</p>
<pre><code>Keys for Duration/ofxTimeline:

Note on OS X the COMMAND key is used, on Linux and Windows the CTRL key is used

Function	Shortcut
Cut Selection	command+x
Copy Selection	command+c
Paste Selection	command+v
Undo	command+z
Redo	shift+command+z
Select all keyframes in Focused track	command+a
Add all keyframes in Focused track to selection	command+shift+a
Delete all selected keyframes	delete or backspace
Nudge keyframes a little	arrow keys
Nudge keyframes a little more	shift+arrow keys
Expand Focused track	alt+e
Collapse all tracks	alt+c
Evenly distribute track sizes	alt+shift+c</code></pre>
<p>Sped things up by taking off vertical sync and smoothing too, 30fps. Did kaleidoscope, little bugs I think...turned the update into a proper switch statement, really improved performance! All good, enough for tonight....</p>
<h4 id="rd-june-2013">23rd June 2013</h4>
<p>OK first thing to do is to take over all the Kinect stuff from:</p>
<p>cariad/reactickles/oF/openFrameworks-develop/apps/zHarp/006withMemoLogic</p>
<p>So taking that over now....Making it all in:</p>
<p>OF/openFrameworks-develop/apps/HAndLGreenpeace/008NewKinectAndPsychBear</p>
<p>Taking over the code, adding the display to the blank scene.... Trying to get the saving working....got it working - it was the bad characters! : and &amp;. That's working, now neatening up the gui screen, adding a blank screen and taking out pointless Kinect modes. OK thats nice, now lets get the psych fur working...All in and the gui in too!</p>
<p>OF/openFrameworks-develop/apps/HAndLGreenpeace/009ShatterExperiment</p>
<p>trying shatter...trying to make it work but there seems to be a conflict when I try to include box2d. Hmmm</p>
<p>all i had to do was change shatter.h to :</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp">include <span class="st">"ofMain.h"</span>
include <span class="st">"ofxOpenCv.h"</span>
include <span class="st">"ofxBox2D.h"</span></code></pre>
<p>from:</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp">include <span class="st">"ofMain.h"</span>
include <span class="st">"ofxBox2D.h"</span>
include <span class="st">"ofxOpenCv.h"</span></code></pre>
<p>Via OF Forum post: http://forum.openframeworks.cc/index.php?topic=7165.0 :</p>
<p>paulf london Posts: 22 Re: Weird codeblocks 007 build errors Reply #5 on: April 05, 2012, 02:12:39 PM in testApp.h having #include "ofxOpenCv.h" at the top of my include list solved the issue for me</p>
<p>Crazy... OK. got that working, but way too slow...</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp"><span class="dt">float</span> timeSinceLastShatter = ofGetElapsedTimef() - timeOfLastShatter;

<span class="kw">if</span>(timeSinceLastShatter &gt; <span class="fl">10.f</span>){ <span class="co">//every 2 seconds make some more....</span>
    <span class="dt">float</span> shatterWidth = theFBO.getWidth();
    <span class="dt">float</span> shatterHeight = theFBO.getHeight();
    <span class="dt">float</span> contourWidth = aContourFinder-&gt;getWidth();
    <span class="dt">float</span> contourHeight = aContourFinder-&gt;getHeight();
    <span class="dt">float</span> widthRatio = shatterWidth/contourWidth;
    <span class="dt">float</span> heightRatio = shatterHeight/contourHeight;
    
    <span class="co">// now just stick some particles on the contour and emit them randomly</span>
    <span class="kw">for</span>(<span class="dt">int</span> i = <span class="dv">0</span>; i &lt; aContourFinder-&gt;nBlobs; i++) {
        <span class="dt">int</span> step = <span class="dv">20</span>;
        
        shape.clear();
        
        <span class="kw">for</span>(<span class="dt">int</span> j = <span class="dv">0</span>; j &lt; aContourFinder-&gt;blobs[i].pts.size(); j+=step) {
            shape.addVertex((aContourFinder-&gt;blobs[i].pts[j].x)*widthRatio,
                            (aContourFinder-&gt;blobs[i].pts[j].y)*heightRatio);
        }
        
        <span class="co">// This is the manual way to triangulate the shape</span>
        <span class="co">// you can then add many little triangles</span>
        <span class="co">// first simplify the shape</span>
        shape.simplify();
        <span class="co">// save the outline of the shape</span>
        ofPolyline outline = shape;
        <span class="co">// resample shape</span>
        ofPolyline resampled = shape.getResampledBySpacing(<span class="dv">256</span>); <span class="co">//dude</span>
        <span class="co">//ofPolyline resampled = shape.getResampledBySpacing(100);</span>
        <span class="co">// triangulate the shape, return am array of triangles</span>
        vector &lt;TriangleShape&gt; tris = triangulatePolygonWithOutline(resampled, outline);
        <span class="co">// add some random points inside</span>
        addRandomPointsInside(shape, <span class="dv">255</span>);

        <span class="co">// now loop through all the tri's and make a box2d triangle</span>
        <span class="kw">for</span> (<span class="dt">int</span> i=<span class="dv">0</span>; i&lt;tris.size(); i++) {
            ofxBox2dPolygon p;
            p.addTriangle(tris[i].a, tris[i].b, tris[i].c);
            p.setPhysics(<span class="fl">1.</span><span class="dv">0</span>, <span class="fl">0.</span><span class="dv">3</span>, <span class="fl">0.</span><span class="dv">3</span>);
            p.setAsEdge(<span class="kw">false</span>);
            <span class="kw">if</span>(p.isGoodShape()) {
                p.create(box2d.getWorld());
                triangles.push_back(p);
            }
        }
        
        <span class="co">// done with shape clear it now</span>
        shape.clear();
    }
    
    timeSinceLastShatter = ofGetElapsedTimef();
}</code></pre>
<p>Let's just spray triangles out from the top of the blobs...like sparkles but with triangles....triangles lame, circles work! Had it running on pete's laptop all lovely...</p>
<h4 id="th-june-2013-7">24th June 2013</h4>
<p>OK, things to try this morning before lunch:</p>
<p>DONE 1. feed in current frame as greyscale for the slitscan DONE 2. try a slitscan mode where I make a spikey slitscan mode - like in divide by zero, going to need OF/openFrameworks-develop/addons/ofxContourUtil from julapy make the triangles shaded? make an ofMesh of it? DONE 3. try the full video, or the mirror vertical/horizontal for the background of the kaleidescope DONE - flock it .4. try a flock attracted to blobs....</p>
<p>OF/openFrameworks-develop/apps/HAndLGreenpeace/010WithSpikyBlobsFlockAndSelfSlitScan</p>
<p>Made that. starting with 1. SELFSLITSCAN - super easy:</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp"><span class="kw">if</span>(timeline.getVideoPlayer(<span class="st">"video"</span>)-&gt;isFrameNew()){
    slitScan.setDelayMap(timeline.getVideoPlayer(<span class="st">"video"</span>)-&gt;getPixelsRef());
    slitScan.addImage(timeline.getVideoPlayer(<span class="st">"video"</span>)-&gt;getPixelsRef());</code></pre>
<p>Next on to spikey mode! Was going to use:</p>
<p>ofxContourUtil-master</p>
<p>From julapy, but it's all in:</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp"><span class="dt">void</span> ofPolyline::simplify(<span class="dt">float</span> tol){</code></pre>
<p>In core, so lets have a go with that...also have:</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp">ofPolyline ofPolyline::getSmoothed(<span class="dt">int</span> smoothingSize, <span class="dt">float</span> smoothingShape)</code></pre>
<p>This is the logic from Divide by Zero:</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp"><span class="co">// contour simplification/manipulation</span>

<span class="dt">int</span> numberOfBlobs = videoContourFinder.blobs.size();

<span class="kw">if</span>(numberOfBlobs &gt; <span class="dv">0</span>){
		<span class="co">//if we have at least one blob</span>
	curve.resize(numberOfBlobs);
	curveSmooth.resize(numberOfBlobs);
	curveSimplify.resize(numberOfBlobs);
	curveCvSimplify.resize(numberOfBlobs);
	<span class="dt">float</span> mx = gui.getValueF(<span class="st">"AURA_SIMPLIFICATION"</span>);
	<span class="dt">float</span> scale1	= mx;
	<span class="dt">float</span> scale2	= mx * <span class="dv">140</span>;
	<span class="dt">float</span> scale3	= mx * <span class="fl">0.</span><span class="dv">1</span>;
	<span class="dt">bool</span> noneSmooth = gui.getValueB(<span class="st">"AURA_IS_SMOOTH"</span>);
	<span class="dt">bool</span> simplifyCV = gui.getValueB(<span class="st">"AURA_IS_CV"</span>);
	<span class="dt">float</span> auraScale = gui.getValueF(<span class="st">"AURA_SCALE"</span>);
	<span class="dt">bool</span> scaleFromStage = gui.getValueB(<span class="st">"AURA_SCALE_FROM_STAGE"</span>);
	
	<span class="kw">for</span>(<span class="dt">int</span> i = <span class="dv">0</span>; i&lt; numberOfBlobs; i++){
		curve[i] = videoContourFinder.blobs[i];
		ofPoint centreOfStage = ofPoint(camWidth/<span class="fl">2.f</span>, camHeight);
		
		<span class="kw">if</span>(scaleFromStage){
			curve[i].scaleBlob(centreOfStage, auraScale); <span class="co">//scale from the base of stage</span>
		}<span class="kw">else</span> {
			curve[i].scaleBlob(curve[i].centroid, auraScale); <span class="co">//else do it from the centroid</span>
		}
		
		<span class="kw">if</span>(noneSmooth){ <span class="co">//smooth it</span>
			cu.smooth( curve[i].pts, curveSmooth[i].pts, scale1 );
		}<span class="kw">else</span>{
				<span class="co">//do nothing.</span>
		}
		
		<span class="kw">if</span>(simplifyCV){
				<span class="co">//cv simplify it</span>
			simplifyDP_openCV( curve[i].pts, curveCvSimplify[i].pts, scale3 );
		}<span class="kw">else</span>{
				<span class="co">//just simplify it</span>
			cu.simplify( curve[i].pts, curveSimplify[i].pts, scale2 );
		}
	}
}</code></pre>
<p>So lets have a look at the demo here:</p>
<p>/Users/joel/Documents/Projects/HellicarAndLewis/greenpeaceArcticGlastonbury2013/OF/openFrameworks-develop/examples/graphics/polylineBlobsExample</p>
<p>Very useful demo....used all the code from demo and ofPolyline in general. Looks nice, ended up doing a simplify down to 10 points.... Let's move onto changing the background for the kaleidoscope...did it with the vertical mirror as the background, pretty.. Downloaded:</p>
<p>https://github.com/jefftimesten/CodeForArt https://github.com/jefftimesten/CodeForArt/tree/master/Chapter004-physics/012-flock/src</p>
<p>Is what i used for Whiteheat anyhow... jefftimesten = jeff crouse. Lets use it again...flock it, too slow too lame no fun... lets do it with particles instead...Looks ok. Going to crack on with the movie....changed name to:</p>
<p>OF/openFrameworks-develop/apps/HAndLGreenpeace/010WithSpikyBlobsParticlesAndSelfSlitScan</p>
<p>Doing the audio now and new film in:</p>
<p>2013_06_24_newFilmAndAudio</p>
<p>Copied in and took over to Pete's computer so he could have a play....he is sequencing...</p>
<h4 id="th-june-2013-8">25th June 2013</h4>
<p>Shower! was lovely. Long drop too.</p>
<p>TODO today: lets do white fur first...looks great...copying over to pete...</p>
<p>DONE 1) kaledscope is always, (2n)+1 : 3,5,7,9,11,13,15,17,19 19 as the limit... which is up to 9 DONE 2) add WHITE FUR to: VERTICALMIRROR, HORIZONTALMIRROR, KALEIDOSCOPE, MIRRORKALEIDOSCOPE, SLITSCANBASIC</p>
<p>copied in the new slitscans:</p>
<ul>
<li>00_ALLBLACK.png</li>
<li>01_ALLWHITE.png</li>
<li>01_random_grid.png</li>
<li>down_to_up.png</li>
<li>left_to_right.png</li>
<li>right_to_left.png</li>
<li>soft_noise.png</li>
<li>Triangle_001.png</li>
<li>Triangle_002.png</li>
<li>Triangle_003.png</li>
<li>Triangle_004.png</li>
<li>Triangle_005.png</li>
<li>up_to_down.png</li>
</ul>
<p>look amazing!</p>
<p>First, kaleidoscope - want always even! timeline.addCurves("star", ofRange(2, 12)); - so just double it... Second, adding white fur.....to VERTICALMIRROR, HORIZONTALMIRROR, KALEIDOSCOPE, MIRRORKALEIDOSCOPE, SLITSCANBASIC</p>
<h4 id="th-june-2013-9">26th June 2013</h4>
<p>Just changed the fur to not have any alpha (white fur that is) also added non-ofxTimeline GUI to everything...</p>
</div>
</div>
<script src="../javascript/jquery-1.8.3.min.js"></script>
<script src="../javascript/jquery-ui-1.9.1.custom.min.js"></script>
<script src="../javascript/bootstrap.js"></script>
<script src="../javascript/jquery.tocify.js"></script>
<script src="../javascript/prettify.js"></script>
<script src="../javascript/tocOpenClose.js"></script>
<script>
        $(function() {

            var toc = $("#toc").tocify({
              selectors: "h2,h3,h4,h5",
              showAndHide: false
            }).data("toc-tocify");

            prettyPrint();
            $(".optionName").popover({ trigger: "hover" });

        });
    </script>
